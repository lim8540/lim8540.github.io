---
category: programmers
tags: [K-digital training, week5_day2, ml_basics, 결정이론]
use_math: true
---
 
## 결정이론 (Decision Theory)

### 결정이론 이란?
- 새로운 값 x가 주어졌을 때, 확률모델 $p(x,t)$에 기반해 최적의 결정(예를 들어 분류)을 내리는 것
    - 추론단계 : 결합확률분포 ($p(x,C_K)$를 직접 구하는 경우도 있음). 이것만 있다면 모든 것을 할 수 있음.
    - 결정단계 : 상황에 대한 확률이 주어졌을 때 어떻게 최적의 결정을 내릴 것인지? 추론단계를 거쳤다면 결정단계는 매우 쉬움
- 예제: X-ray의 이미지로 암 판별
    - x : X-ray이미지
    - $C_1$ : 암인 경우
    - $C_2$ : 암이 아닌 경우
    - $p(C_k \vert x)$의 값을 알기 원함
- 직관적으로 볼 때 $p(C_k \vert x)$를 최대화 시키는 k를 구하는 것이 좋은 결정

### 결정이론 - 이진분류(Binary Classification)
- 결정 영역(decision region)
    - $R_i = \\{ x : pred(x) = C_i \\}$
- 분류 오류 확률(probability of misclassification)
    - $p(mis) = p(x \in R_1, C_2) + p(x \in R_2, C_1) = \int_{R_1}p(x, C_2)dx + \int_{R_2}p(x, C_1)dx$
- 오류를 최소화하려면 다음조건을 만족하는 x를 $R_1$에 할당해야 한다.
    - $p(x, C_1) > p(x, C_2)$
    - $\iff p(C_1 \vert x)p(x) > p(C_2 \vert x)p(x)$
    - $\iff p(C_1 \vert x) > p(C_2 \vert x)$

### Multi Class일 경우
- $p(correct) = \sum_{k=1}^Kp(x \in R_k, C_k) = \sum_{k=1}^K\int_{R_k}p(x,C_k)dx$
- $ = pred(x) = argmax_kp(C_k \vert x)$

### 결정이론의 목표(분류의 경우)
- 결합확률분포 $p(x, C_k)$가 주어졌을 때, 최적의 결정영역들 $R_1, ..., R_k$를 찾는 것.
- $\hat C(x)$를 $x$가 주어졌을 때 예측값(1,...,K 중의 값)을 돌려주는 함수라고 하자.
    - $x \in R_j \iff \hat C(x) = j$
- 결합확률분포 $p(x, C_k)$가 주어졌을 때, 최적의 함수 $\hat C(x)$를 찾는 것.

### 기대손실 최소화(Minimizing the Expected Loss)
- 모든 결정이 동일한 리스크를 갖는 것은 아님.
    - 암이 아닌데 암인 것으로 진단
    - 암이 맞는데 암이 아닌 것으로 진단
- 손실행렬(loss matrix)
    - $L_{kj}$ : $C_k$에 속하는 $x$를 $C_j$로 분류할 때 발생하는 손실(또는 비용)
    - 데이터에 대한 모든 지식이 확률분포로 표현되고 있는 것을 기억할 것. 한 데이터샘플 x의 실제 클래스를 결정론적으로 알고 있는 것이 아니라 그것의 확률만을 알 수 있다고 가정한다. 즉, 우리가 관찰할 수 있는 샘플(예를들어, 암을 가진 환자의 X-Ray이미지)은 확률분포 $p(x, C_k)$를 통해서 생성된 것이라고 간주한다. 따라서, 손실행렬 L이 주어졌을 때, 다음과 같은 기대손실을 최소화하는 것을 목표로 할 수 있다.
    - $E[L] = \sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx$
    - $\hat C(x)$를 활용햐여 위의 식을 바꾸면 다음과 같다.
    - $\int_x\sum_{k=1}^KL_{k\hat C(x)}p(x,C_k)dx = \int_x(\sum_{k=1}^KL_{k\hat C(x)}p(C_k \vert x))p(x)dx$
    - 이렇게 표현된 $E[L]$는 $\hat C(x)$의 범함수(functional)이고이 범함수를 최소화시키는 함수 $\hat C(x)$를 찾으면 된다.
        - $ \hat C(x) = argmin \sum_{k=1}^KL_{kj}p(C_k \vert x)$
    - 만약에 손실행렬이 0-1 loss인 경우(주대각선 원소들은 0 나머지는 1)
        - $ \hat C(x) = argmin[ \\{ \sum_{k=1}^Kp(C_k \vert x) \\} - p(C_j \vert x)]$
        - $ = argmin (1 - p(C_j \vert x)) = argmax p(C_j \vert x)$

### 결정이론 - 회귀문제의 경우
- 목표값 $t \in R$
- 손실함수 : $ L(t, y(x)) = \\{y(x) - t\\}^2$
    - $F[y] = E[L] = \int_R \int_X\\{ y(x) - t\\}^2p(x,t)dxdt$
    - $ = \int_X (\int_R\\{ y(x) - t\\}^2p(x,t)dt)dx $
    - $ = \int_X (\int_R\\{ y(x) - t\\}^2p(t \vert x)dt)p(x)dx $
- x를 위한 최적의 예측값은 $y(x) = E_t[t \vert x]$이다.(증명생략)

### 결정문제를 위한 몇 가지 방법들

#### 분류문제의 경우
- 확률문제에 의존하는 경우
    - 생성모델(generative model) : 먼저 각 클래스 $C_k$에 대해 분포 $p(x \vert C_k)$와 사전확률 $p(C_k)$를 구한다음 베이즈 정리를 사용해서 사후확률 $p(C_k \vert x)$를 구한다. 
    - $ p(C_k \vert x) = \frac {p(x \vert C_k)p(C_k)}{p(x)}
    - p(x)는 다음과 같이 구한다
    - $p(x) = \sum_kp(x \vert C_k)p(C_k)$
    - 사후확률이 주어졌기 때문에 분류를 위한 결정은 쉽게 이루어질 수 있다. 결합분포에서 데이터를 샘플링해서 "생성"할 수 있으므로 이런 방식을 생성모델이라고 부른다.
    - 식별모델(discriminative model) : 모든 분포를 다 계산하지 않고 오직 사후확률 $p(C_k \vert x)$를 구한다. 위와 동일하게 결정이론을 적용할 수 있다.
- 판별함수에 의존하는 경우
    - 판별함수(discriminant function) : 입력 x를 클래스로 할당하는 판별함수를 찾는다. 확률값은 계산하지 않는다.

#### 회귀문제의 경우
- 결합분포 $p(x,t)$를 구하는 추론(inference)문제를 먼저 푼다음 조건부확률분포 $p(t \vert x)$를 구한다. 그리고 주변화(marginalize)를 통해 $E_t[t \vert x]$를 구한다.
- 조건부확률분포 $p(t \vert x)$를 구하는 추론문제를 푼 다음 주변화(marginalize)를 통해 $E_t[t \vert x]$를 구한다.
- $y(x)$를 직접적으로 구한다.