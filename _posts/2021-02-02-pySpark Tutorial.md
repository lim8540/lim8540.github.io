---
category: programmers
tags: [K-digital training, week9_day2, ml_basics, pyspark ]
use_math: true
---

## pySpark Tutorial with Colab
PySparkì„ ë¡œì»¬ë¨¸ì‹ ì— ì„¤ì¹˜í•˜ê³  ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ê¸° ë³´ë‹¤ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì—ˆê³  ì¢‹ì€ í•˜ë“œì›¨ì–´ë¥¼ ì œê³µí•´ì£¼ëŠ” Google Colabì„ í†µí•´ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

ì´ë¥¼ ìœ„í•´ pysparkê³¼ py4J íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤. Py4JíŒ¨í‚¤ì§€ëŠ” íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì´ ìë°”ê°€ìƒë¨¸ì‹ ìƒì˜ ì˜¤ë¸Œì íŠ¸ë“¤ì„ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. Local Standardalone Sparkë¥¼ ì‚¬ìš©í•œë‹¤.


```
!pip install pyspark==3.0.1 py4j==0.10.9
```

    Collecting pyspark==3.0.1
    [?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204.2MB 66kB/s 
    [?25hCollecting py4j==0.10.9
    [?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 47.7MB/s 
    [?25hBuilding wheels for collected packages: pyspark
      Building wheel for pyspark (setup.py) ... [?25l[?25hdone
      Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=fca352d74764aefb8a09781c034a614b6d6d368403f6797e5fee21b4d312e888
      Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f
    Successfully built pyspark
    Installing collected packages: py4j, pyspark
    Successfully installed py4j-0.10.9 pyspark-3.0.1



```
!ls -tl
```

    total 4
    drwxr-xr-x 1 root root 4096 Jan 20 17:27 sample_data



```
!ls -tl sample_data
```

    total 55504
    -rw-r--r-- 1 root root 18289443 Jan 20 17:27 mnist_test.csv
    -rw-r--r-- 1 root root 36523880 Jan 20 17:27 mnist_train_small.csv
    -rw-r--r-- 1 root root   301141 Jan 20 17:27 california_housing_test.csv
    -rw-r--r-- 1 root root  1706430 Jan 20 17:27 california_housing_train.csv
    -rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json
    -rwxr-xr-x 1 root root      930 Jan  1  2000 README.md


**Spark Session**: Spark Sessionì€ Spark 2.0ë¶€í„° ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ë¡œ ì‚¬ìš©ëœë‹¤. ê·¸ ì´ì „ì—ëŠ” SparkContextê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. SparkSessionì„ ì´ìš©í•´ RDd, ë°ì´í„° í”„ë ˆì„ ë“±ì„ ë§Œë“ ë‹¤. SparkSessionì€ SparkSession.builderë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒì„±í•˜ë©° ë‹¤ì–‘í•œ í•¨ìˆ˜ë“¤ì„ í†µí•´ ì„¸ë¶€ ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤. 


```python
from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local[*]")\  #ë§ˆìŠ¤í„°ì˜ ì¸ìë¡œëŠ” ì‚¬ìš©í•˜ê³  ì‹¶ì€ spark clusterí˜¸ìŠ¤íŠ¸ ì´ë¦„ì„ ì¤Œ
        .appName('pySpark_Tutorial')\
        .getOrCreate()
```


```python
spark
```





    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://216bf774aeac:4040">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.0.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>pySpark_Tutorial</code></dd>
    </dl>
</div>

    </div>




### Python ê°ì²´ë¥¼ RDDë¡œ ë³€í™˜í•´ë³´ê¸°
#### 1> Python ë¦¬ìŠ¤íŠ¸ ìƒì„±


```python
name_list_json = ['{"name" : "keeyong"}', '{"name" : "benjamin"}', '{"name" : "claire"}']
```


```python
for n in name_list_json:
  print(n)
```

    {"name" : "keeyong"}
    {"name" : "benjamin"}
    {"name" : "claire"}



```python
import json
for n in name_list_json:
  jn = json.loads(n)
  print(jn["name"])
```

    keeyong
    benjamin
    claire


#### 2> íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ë³€í™˜, RDDë¡œ ë³€í™˜ë˜ëŠ” ìˆœê°„ Spark í´ëŸ¬ìŠ¤í„°ì˜ ì„œë²„ë“¤ì— ë°ì´í„°ê°€ ë‚˜ëˆ„ì–´ ì €ì¥ë¨(íŒŒí‹°ì…˜). ë˜í•œ Lazy Executionì´ ëœë‹¤ëŠ” ì  ê¸°ì–µ


```python
rdd = spark.sparkContext.parallelize(name_list_json)
```


```python
rdd
```




    ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262




```python
rdd.count()
```




    3




```python
parsed_rdd = rdd.map(lambda el : json.loads(el))
```


```python
parsed_rdd
```




    PythonRDD[2] at RDD at PythonRDD.scala:53




```python
parsed_rdd.collect()
```




    [{'name': 'keeyong'}, {'name': 'benjamin'}, {'name': 'claire'}]




```python
parsed_name_rdd = rdd.map(lambda el : json.loads(el)["name"])
```


```python
parsed_name_rdd.collect()
```




    ['keeyong', 'benjamin', 'claire']



### íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê¸°


```python
from pyspark.sql.types import StringType

df = spark.createDataFrame(name_list_json, StringType())
```


```python
df.count()
```




    3




```python
df.printSchema()
```

    root
     |-- value: string (nullable = true)
    



```python
df.select('*').collect()  # * : ëª¨ë“  ê°ì²´ ë¶ˆëŸ¬ì˜¤ê¸°
```




    [Row(value='{"name" : "keeyong"}'),
     Row(value='{"name" : "benjamin"}'),
     Row(value='{"name" : "claire"}')]




```python
df.select('value').collect()
```




    [Row(value='{"name" : "keeyong"}'),
     Row(value='{"name" : "benjamin"}'),
     Row(value='{"name" : "claire"}')]




```python
from pyspark.sql import Row

row = Row("name") # Or some other column name
df_name = parsed_name_rdd.map(row).toDF()
```


```python
df_name.printSchema()
```

    root
     |-- name: string (nullable = true)
    



```python
df_name.select('name').collect()
```




    [Row(name='keeyong'), Row(name='benjamin'), Row(name='claire')]

