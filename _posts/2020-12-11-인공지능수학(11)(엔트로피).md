---
category: programmers
tags: [K-digital training, week2_day5, 인공지능 수학]
use_math: true
---

## 엔트로피

- 자기정보(self-information) : $i(A)$
    - A : 사건
    - $ i(A) = {log_b({1\over {P(A)}})} = -log_bP(A)$
    - 확률이 높은 사건:
        - 정보가 많지 않음
        - 예 : 도둑이 들었는데 개가 짖는 경우보다 도둑이 들었는데 개가 안 짖는 경우 더 많은 정보를 포함하고 있음
    - 정보의 단위
        - b = 2 : bits
        - b = e : nats
        - b = 10 : hartley

    - 특성
        - $ i(AB) ={log_b({1\over {P(A)P(B)}})} = {log_b({1\over {P(A)}})} + {log_b({1\over {P(B)}})} = i(A) + i(B)$
        - 따라서, 두 사건이 동시에 있어났을 때, 자기정보는 각 자기정보의 합과 같다.
        - $ P(H) = {1\over8}, P(T) = {7\over8} $
            - $ i(H) = -log_bP(H) = -log_2{1 \over 8} = 3비트$
            - $ i(H) = -log_bP(T) = -log_2{7 \over 8} = 0.193비트$

- 엔트로피(entropy)
    - 자기 정보의 평균
    - $ H(X) = \sum_J P(A_j)i(A_j) = - \sum_jP(A_j)log_2 P(A_j) $
    - 특성
        - $ 0 \le H(X) \le log_2K $ ( K : 사건의 수)
            - 엔트로피 $H(X)$의 최대값은 $P(A_j)$가 모두 ${1\K}$인 경우이고 그때의 값이 $log_2K$이다.

## 교차 엔트로피
- 확률분포 P와 Q
    - $ S = {A_j} $(사건 S)
    - $ P(A_j)$ : 확률분포 P에서 사건 $A_j$가 발생할 확률
    - $ Q(A_j)$ : 확률분포 Q에서 사건 $A_j$가 발생할 확률
    - $ i(A_j)$ : 확률분포 Q에서 사건 $A_j$의 자기정보
        - $ i(A_j) = -log_2Q(Aj)$
        - 자기정보는 $A_j$를 표현하는 비트수
        - 잘못된 확률분포 Q를 사용하게 되면, 실제 최적의 비트수를 사용하지 못하게 됨.

- $H(P,Q)$
    - 집합 S상에서 확률분포 P에 대한 확률분포 Q의 교차 엔트로피
    - 확률분포 P에서 $i(A_j)$의 평균
        - $ H(P, Q) = \sum_j P(A_j)i(A_j) = - \sum_j P(A_j)log_2Q(A_j) = - \sum_{x \in X}P(X)log_2Q(x) $
        - 이 값은 정확한 확률분포 P를 사용했을 때의 비트 수 보다 크게 됨
            - $ H(P,Q) = - \sum_{x \in X}P(X)log_2Q(x) \ge - \sum_{x \in X}P(X)log_2P(x) = H(P)$
        - 따라서 이 값은 P와 Q가 얼마나 비슷한지를 표현
            - 같으면 $ H(P,Q) = H(P) $
            - 다르면 $ H(P,Q) > H(P) $
- 분류 문제에서의 손실함수
    - 분류문제
        - 주어진 대상이 A인지 아닌지를 판단
        - 주어진 대상이 A, B, C, ... 중 어느 것인지를 판단
    - 기계학습 에서는 주어진 대상이 각 그룹에 속할 확률을 제공
        - 예) [0.8, 0.2]: A일 확률 0.8, 아닐확률 0.2
        - 이 값이 정답인 [1.0 ,0.0] 과 얼마나 다른지 측정 필요
    - 원하는 답 $P = [p_1, p_2, ..., p_n], p_1 + p_2 + ... + p_n = 1$
    - 제시된 답 $Q = [q_1, q_2, ..., q_n], q_1 + q_2 + ... + q_n = 1$
    - 제곱합
        - $\sum(p_i - q_i)^2$
        - 확률이 다를수록 큰 값을 가짐
        - 하지만 학습 속도 느림
    - 교차 엔트로피 H(P, Q)
        - 확률이 다를수록 큰 값을 가짐
        - 학습 속도 빠름
        - 분류 문제에서 주로 교차 엔트로피 사용
    - 참고) 분류문제에서 원하는 답
        - $P = [p_1, p_2, ..., p_n]$에서 $p_i$중 하나만 1이고, 나머지는 다 0임. 즉, 엔트로피는 0 (H(P) = 0)
        - $p_k = 1.0$ 이라고 하면, $q_k$의 값이 최대한 커지는 방향으로 학습 진행
        - 예시) 
            - S = ${A, B}$
            - P = $[1, 0]$
            - 예측 $Q(x)$
                - $[0.8, 0.2]: Q(A) = 0.8. Q(B) = 0.2$
                    - $H(P, Q) = - \sum_{x \in X}log_2Q(x) = -1 \times log_20.8 = 0.3219$
                - $[0.5, 0.5]: Q(A) = 0.5. Q(B) = 0.5$
                    - $H(P, Q) = - \sum_{x \in X}log_2Q(x) = -1 \times log_20.5 = 1$
                - $[0.8, 0.2]: Q(A) = 0.8. Q(B) = 0.2$
                    - $H(P, Q) = - \sum_{x \in X}log_2Q(x) = -1 \times log_20.2 = 2.32$
            - H(P,Q)가 가장 작은 [0.8, 0.2]가 가장 정답에 가까움을 알 수 있다.

```python
import numpy as np
def crossentropy(P,Q):
    return sum([-P[i] * np.log2(Q[i]) for i in range(len(P))])
P = [1,0]
Q = [0.8, 0.2]
print(crossentropy(P,Q))
...
```



