---
category: programmers
tags: [K-digital training, week5_day2, ml_basics, 선형회귀]
use_math: true
---
 

## 선형회귀(Linear Regresion)

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
```

먼저, 주어진 데이터를 직선을 사용해 모델링 하는 방법을 살펴본다. 직선함수는 다음고 같은 형태를 가진다.   
$y = ax + b$   
여기서 a는 기울기(slope)이고 b는 y절편 (intercept)라고 불린다.   
아래 그래프는 기울기가 2이고 y절편이 -5인 직선으로부터 생성된 데이터를 보여준다.


```python
rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x,y)
```




    <matplotlib.collections.PathCollection at 0x7fcb82128ed0>




![ML_Basics(Linear Regression)_2_1](https://user-images.githubusercontent.com/51064261/103624985-a99bd300-4f7d-11eb-8d0c-10b267c0fbf9.png)



Scikit-Learn의 **LinearRegression** estimator를 사용해서 위 데이터를 가장 잘 표현하는 직선을 찾을 수 있다.


```python
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept = True)

#x[:,np.newaxis]는 x의 차원을 늘려주는 역할을 함. x의 shape는 (50,0) 새로운 shape는 (50,1)
model.fit(x[:, np.newaxis], y)

xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(x,y)
plt.plot(xfit, yfit)
```




    [<matplotlib.lines.Line2D at 0x7fcb83649690>]




![ML_Basics(Linear Regression)_4_1](https://user-images.githubusercontent.com/51064261/103625268-14e5a500-4f7e-11eb-8dd7-320ed2b25755.png)



모델 학습이 끝난 후 학습된 파라미터들은 model."파라미터이름"\_의 형태로 저장된다. 기울기와 y절편은 아래와 같이 출력할 수 있다.


```python
print("Model slope: ", model.coef_[0])
print("Model intercept: ", model.intercept_)
```

    Model slope:  2.027208810360695
    Model intercept:  -4.998577085553202


LinearRegression estimator는 위의 예제와 같은 1차원 입력뿐만 아니라 다차원 입력을 사용한 선형모델을 다룰 수 있다. 다차원 선형모델은 다음과 같은 형태를 가진다.   
$y = a_0 + a_1x_1 + a_2x_2 + ...$   
기하학적으로 이것은 hyper-plane으로 데이터를 표현하는 것이라고 말할 수 있다.


```python
rng = np.random.RandomState(1)
rng.rand(100,3)
```




    array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],
           [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],
           [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],
           [5.38816734e-01, 4.19194514e-01, 6.85219500e-01],
           [2.04452250e-01, 8.78117436e-01, 2.73875932e-02],
           [6.70467510e-01, 4.17304802e-01, 5.58689828e-01],
           [1.40386939e-01, 1.98101489e-01, 8.00744569e-01],
           [9.68261576e-01, 3.13424178e-01, 6.92322616e-01],
           [8.76389152e-01, 8.94606664e-01, 8.50442114e-02],
           [3.90547832e-02, 1.69830420e-01, 8.78142503e-01],
           [9.83468338e-02, 4.21107625e-01, 9.57889530e-01],
           [5.33165285e-01, 6.91877114e-01, 3.15515631e-01],
           [6.86500928e-01, 8.34625672e-01, 1.82882773e-02],
           [7.50144315e-01, 9.88861089e-01, 7.48165654e-01],
           [2.80443992e-01, 7.89279328e-01, 1.03226007e-01],
           [4.47893526e-01, 9.08595503e-01, 2.93614148e-01],
           [2.87775339e-01, 1.30028572e-01, 1.93669579e-02],
           [6.78835533e-01, 2.11628116e-01, 2.65546659e-01],
           [4.91573159e-01, 5.33625451e-02, 5.74117605e-01],
           [1.46728575e-01, 5.89305537e-01, 6.99758360e-01],
           [1.02334429e-01, 4.14055988e-01, 6.94400158e-01],
           [4.14179270e-01, 4.99534589e-02, 5.35896406e-01],
           [6.63794645e-01, 5.14889112e-01, 9.44594756e-01],
           [5.86555041e-01, 9.03401915e-01, 1.37474704e-01],
           [1.39276347e-01, 8.07391289e-01, 3.97676837e-01],
           [1.65354197e-01, 9.27508580e-01, 3.47765860e-01],
           [7.50812103e-01, 7.25997985e-01, 8.83306091e-01],
           [6.23672207e-01, 7.50942434e-01, 3.48898342e-01],
           [2.69927892e-01, 8.95886218e-01, 4.28091190e-01],
           [9.64840047e-01, 6.63441498e-01, 6.21695720e-01],
           [1.14745973e-01, 9.49489259e-01, 4.49912133e-01],
           [5.78389614e-01, 4.08136803e-01, 2.37026980e-01],
           [9.03379521e-01, 5.73679487e-01, 2.87032703e-03],
           [6.17144914e-01, 3.26644902e-01, 5.27058102e-01],
           [8.85942099e-01, 3.57269760e-01, 9.08535151e-01],
           [6.23360116e-01, 1.58212428e-02, 9.29437234e-01],
           [6.90896918e-01, 9.97322850e-01, 1.72340508e-01],
           [1.37135750e-01, 9.32595463e-01, 6.96818161e-01],
           [6.60001727e-02, 7.55463053e-01, 7.53876188e-01],
           [9.23024536e-01, 7.11524759e-01, 1.24270962e-01],
           [1.98801338e-02, 2.62109869e-02, 2.83064880e-02],
           [2.46211068e-01, 8.60027949e-01, 5.38831064e-01],
           [5.52821979e-01, 8.42030892e-01, 1.24173315e-01],
           [2.79183679e-01, 5.85759271e-01, 9.69595748e-01],
           [5.61030219e-01, 1.86472894e-02, 8.00632673e-01],
           [2.32974274e-01, 8.07105196e-01, 3.87860644e-01],
           [8.63541855e-01, 7.47121643e-01, 5.56240234e-01],
           [1.36455226e-01, 5.99176895e-02, 1.21343456e-01],
           [4.45518785e-02, 1.07494129e-01, 2.25709339e-01],
           [7.12988980e-01, 5.59716982e-01, 1.25559802e-02],
           [7.19742797e-02, 9.67276330e-01, 5.68100462e-01],
           [2.03293235e-01, 2.52325745e-01, 7.43825854e-01],
           [1.95429481e-01, 5.81358927e-01, 9.70019989e-01],
           [8.46828801e-01, 2.39847759e-01, 4.93769714e-01],
           [6.19955718e-01, 8.28980900e-01, 1.56791395e-01],
           [1.85762022e-02, 7.00221437e-02, 4.86345111e-01],
           [6.06329462e-01, 5.68851437e-01, 3.17362409e-01],
           [9.88616154e-01, 5.79745219e-01, 3.80141173e-01],
           [5.50948219e-01, 7.45334431e-01, 6.69232893e-01],
           [2.64919558e-01, 6.63348344e-02, 3.70084198e-01],
           [6.29717507e-01, 2.10174010e-01, 7.52755554e-01],
           [6.65364814e-02, 2.60315099e-01, 8.04754564e-01],
           [1.93434283e-01, 6.39460881e-01, 5.24670309e-01],
           [9.24807970e-01, 2.63296770e-01, 6.59610907e-02],
           [7.35065963e-01, 7.72178030e-01, 9.07815853e-01],
           [9.31972069e-01, 1.39515730e-02, 2.34362086e-01],
           [6.16778357e-01, 9.49016321e-01, 9.50176119e-01],
           [5.56653188e-01, 9.15606350e-01, 6.41566209e-01],
           [3.90007714e-01, 4.85990667e-01, 6.04310483e-01],
           [5.49547922e-01, 9.26181427e-01, 9.18733436e-01],
           [3.94875613e-01, 9.63262528e-01, 1.73955667e-01],
           [1.26329519e-01, 1.35079158e-01, 5.05662166e-01],
           [2.15248053e-02, 9.47970211e-01, 8.27115471e-01],
           [1.50189807e-02, 1.76196256e-01, 3.32063574e-01],
           [1.30996845e-01, 8.09490692e-01, 3.44736653e-01],
           [9.40107482e-01, 5.82014180e-01, 8.78831984e-01],
           [8.44734445e-01, 9.05392319e-01, 4.59880266e-01],
           [5.46346816e-01, 7.98603591e-01, 2.85718852e-01],
           [4.90253523e-01, 5.99110308e-01, 1.55332756e-02],
           [5.93481408e-01, 4.33676349e-01, 8.07360529e-01],
           [3.15244803e-01, 8.92888709e-01, 5.77857215e-01],
           [1.84010202e-01, 7.87929234e-01, 6.12031177e-01],
           [5.39092721e-02, 4.20193680e-01, 6.79068837e-01],
           [9.18601778e-01, 4.02024891e-04, 9.76759149e-01],
           [3.76580315e-01, 9.73783538e-01, 6.04716101e-01],
           [8.28845808e-01, 5.74711505e-01, 6.28076198e-01],
           [2.85576282e-01, 5.86833341e-01, 7.50021764e-01],
           [8.58313836e-01, 7.55082188e-01, 6.98057248e-01],
           [8.64479430e-01, 3.22680997e-01, 6.70788791e-01],
           [4.50873936e-01, 3.82102752e-01, 4.10811350e-01],
           [4.01479583e-01, 3.17383946e-01, 6.21919368e-01],
           [4.30247271e-01, 9.73802078e-01, 6.77800891e-01],
           [1.98569888e-01, 4.26701009e-01, 3.43346240e-01],
           [7.97638804e-01, 8.79998289e-01, 9.03841956e-01],
           [6.62719812e-01, 2.70208262e-01, 2.52366702e-01],
           [8.54897943e-01, 5.27714646e-01, 8.02161084e-01],
           [5.72488517e-01, 7.33142525e-01, 5.19011627e-01],
           [7.70883911e-01, 5.68857991e-01, 4.65709879e-01],
           [3.42688908e-01, 6.82093484e-02, 3.77924179e-01],
           [7.96260777e-02, 9.82817114e-01, 1.81612851e-01]])




```python
X = 10 * rng.rand(100,3)
#y = 100 by 3 행렬과 크기 3 열벡터의 행렬곱
y = 0.5 + np.dot(X, [1.5, -2., 1.])

y
```




    array([ -3.37986709,  10.74767891,  -6.09131716,   8.43244545,
            20.01412846,   4.1934773 ,  -0.35094038,   2.06439823,
            -3.49599876,  -4.43407273,   2.8045874 ,  11.73613282,
             1.84545434,  18.62297315,   7.66471539,   0.28201632,
            -7.04995534,   8.21217837,  10.46412826,   1.08135752,
            -3.20686456,   9.020925  ,   2.80718498,  -9.07917586,
            -8.38474873,   7.27658939,   6.68275817,   8.61155789,
            15.74770923,   2.81707056,  -5.40078872,  -0.95319755,
            16.24978318,  19.35543153,  -4.67346522,  14.97781827,
             9.02197798,   2.9820238 ,  12.62515506,  -0.28833735,
             1.24377741,  10.86442283, -11.98041792,  -2.54929231,
             0.76504876,  17.51417234,  -5.70313302,   6.26514634,
            -3.51841988,   1.35496684,  -2.08336832,  -5.88361643,
            10.86096322,  -4.9367356 ,  -3.88796154,  14.80808349,
            -1.1477796 ,  14.46665226,   0.216072  ,  -0.2871369 ,
             0.32925132,   8.34530304,   5.05066835,  12.7567254 ,
           -14.70672142,   4.87199645,  -8.65569584,  11.98873887,
            -7.85813283,   5.97030505,   7.85318327,  14.14196923,
            -1.12908267, -12.94332059,  -0.21645362,  -6.88756954,
            10.37750735,   1.5342091 ,  16.37077611,   1.52484819,
            13.39370183,  -3.83864459,   1.28380699,  15.36109386,
            -3.1072014 ,  11.8134319 ,   2.18534994,  19.0904709 ,
             3.3314668 ,  10.06412076,   8.62160201,  -8.14847679,
             6.97457881,  -2.33743702,   5.3325264 ,   9.44047459,
            -4.21867229,  -0.07298716,  15.37233961,   2.8022421 ])




```python
y.shape
```




    (100,)




```python
model.fit(X,y)
print(model.intercept_)
print(model.coef_)
```

    0.5000000000000018
    [ 1.5 -2.   1. ]


y값들은 랜덤하게 생성된 3차원의 x값과 계수들([1.5, -2., 1.])을 곱함으로써 생성되었는데, linear regression을 통해서 이 계수들을 계산해낼 수 있다는 것을 알 수 있다.
   
   만약 데이터가 선형적인 관계를 가지고 있지 않다면?

## 선형 기저함수 모델(Linear Basis function Models)
비선형데이터를 선형함수로 모델링하는 한가지 망법은 기저함수(basis function)을 사용하는 것이다.   
예를 들어, 다음과 같은 선형함수를 사용한다고 하자.   
$ y = a_0 + a_1x_1 + a_2x_2 + a_3x_3 + ...$   
여기서 $x_1, x_2, x_3$등을 1차원 x로 부터 생성할 수 있다. $(x_n = f_n(x))$. $f_n$을 기저함수라고 부른다.   
만약 $f_n(x) = x^n$라는 기저함수를 사용하면 최종적인 모델은 다음과 같을 것이다.   
$ y = a_0 + a_1x_1 + a_2x_2^2 + a_3x_3^3 + ...$   
이 모델은 여전히 계수$(a_n)$에 관해서는 선형함수임을 기억하자. 따라서 1차원 변수인 x를 기저함수를 통해 다차원으로 확장시킴으로써 우리는 여전히 선형모델(linear regression)을 사용할 수 있게 된다.


### 다항 기저함수(Polynoial Basis Functions)
$f_n(x) = x^n$의 형태의 함수를 다항 기저함수라고 부른다. Scikit-Learn은 PolynomialFeatures이라는 transformer를 이미 포함하고 있다.


```python
from sklearn.preprocessing import PolynomialFeatures
x = np.array([2, 3, 4])
poly = PolynomialFeatures(3, include_bias = False)
poly.fit_transform(x[:, None])
```




    array([[ 2.,  4.,  8.],
           [ 3.,  9., 27.],
           [ 4., 16., 64.]])



PolynomialFeatures가 1차원 array를 3차원 array로 변환한 것을 볼 수 있다. 이렇게 변환된 데이터를 선형모델에 적용할 수 있다.   
   
   7차원 변환을 적용해보자.


```python
from sklearn.pipeline import make_pipeline
poly_model = make_pipeline(PolynomialFeatures(7),
                          LinearRegression())
```

다차원 변환을 사용하면 복잡한 데이터를 모델링할 수 있게 된다. 예를 들어 sine함수를 사용해서 데이터를 생성하고 모델링 해보자.


```python
rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = np.sin(x) + 0.1 * rng.randn(50)
plt.scatter(x,y)
```




    <matplotlib.collections.PathCollection at 0x7fcb8388af10>




![ML_Basics(Linear Regression)_19_1](https://user-images.githubusercontent.com/51064261/103625297-2169fd80-4f7e-11eb-91b4-e92463bfdf4d.png)



```python
poly_model.fit(x[:, np.newaxis], y)
yfit = poly_model.predict(xfit[:, np.newaxis])

plt.scatter(x,y)
plt.plot(xfit, yfit)
```




    [<matplotlib.lines.Line2D at 0x7fcb8389d110>]




![ML_Basics(Linear Regression)_20_1](https://user-images.githubusercontent.com/51064261/103625337-2fb81980-4f7e-11eb-8962-887ace82447f.png)


### 가우시안 기저함수(Gaussian Basis Functions)
다항 기저함수 외에 다른 기저함수를 사용해보자. 가우시안 기저함수는 다음과 같이 정의된다.   
$exp(- \frac {(x-u_j)^2}{2s^2})$   
$u_j$는 함수의 위치, s는 폭을 결정한다. 주어진 데이터를 여러개의 가우시안 기저함수들의 합으로 표현하려고 시도할 수 있다.


```python
from sklearn.base import BaseEstimator, TransformerMixin

class GaussianFeatures(BaseEstimator, TransformerMixin):
    """Uniformly spaced Gaussian features for one-dimensional input"""
    
    def __init__(self, N, width_factor=2.0):
        self.N = N
        self.width_factor = width_factor
    
    @staticmethod
    def _gauss_basis(x, y, width, axis=None):
        arg = (x - y) / width
        return np.exp(-0.5 * np.sum(arg ** 2, axis))
        
    def fit(self, X, y=None):
        # create N centers spread along the data range
        self.centers_ = np.linspace(X.min(), X.max(), self.N)
        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])
        return self
        
    def transform(self, X):
        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,
                                 self.width_, axis=1)
    
gauss_model = make_pipeline(GaussianFeatures(20),
                            LinearRegression())
gauss_model.fit(x[:, np.newaxis], y)
yfit = gauss_model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10);

```


![ML_Basics(Linear Regression)_22_0](https://user-images.githubusercontent.com/51064261/103625373-3e063580-4f7e-11eb-95b5-f37b05a24a77.png)


### 규제화(Regularization)
기저함수를 사용함으로써 복잡한 데이터를 모델링할 수 있게 되었지만 조심하지 않는다면 over-fitting이라는 다른 심각한 무제를 만날 수 있다! 예를들어, 너무 많은 개수의 가우시안 기저함수를 사용하게 되면 다음과 같이 될 수 있다.


```python
model = make_pipeline(GaussianFeatures(30),
                            LinearRegression())
model.fit(x[:, np.newaxis], y)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10)
plt.ylim(-1.5, 1.5)
```




    (-1.5, 1.5)




![ML_Basics(Linear Regression)_24_1](https://user-images.githubusercontent.com/51064261/103625379-3e9ecc00-4f7e-11eb-9e26-7375b0333e7b.png)



이 예제에서는 30개의 기저함수가 사용되었는데 모델이 필요이상으로 flexible해져서 데이터가 없는 곳에서는 극단적인 값을 가지는 것을 볼 수 있다. 기저함수의 계수들은 다음과 같이 확인할 수 있다.


```python
def basis_plot(model, title=None):
    fig, ax = plt.subplots(2, sharex=True)
    model.fit(x[:, np.newaxis], y)
    ax[0].scatter(x, y)
    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))
    
    if title:
        ax[0].set_title(title)

    ax[1].plot(model.steps[0][1].centers_,
               model.steps[1][1].coef_)
    ax[1].set(xlabel='basis location',
              ylabel='coefficient',
              xlim=(0, 10))
    
model = make_pipeline(GaussianFeatures(30), LinearRegression())
basis_plot(model)
```


![ML_Basics(Linear Regression)_26_0](https://user-images.githubusercontent.com/51064261/103625381-3f376280-4f7e-11eb-9067-1d49bbf08013.png)


위 두번째 그래프는 각각의 가우시안 기저함수의 크기(계수값)을 보여주고 있다. Over-fitting이 일어나는 영역에서는 인접한 기저함수들의 값이 극단으로 가면서 서로 상쇄하는 현상이 일어난다. 따라서 큰 계수값에 대해 penalty를 부여해서 over-fitting을 어느 정도 극복할 수 있을 것이다. 이러한 penalty를 regularization이라 부른다.

### Ridge regression(L2 Regularization)
가장 자주 쓰이는 형태의 regularization은 ridge regression(L2 regularization)이고 다음과 같이 정의된다.   
$ P = \alpha \sum_{n=1}^N \theta_n^2$   
여기서 $\alpha$는 regularization의 강도를 조절하는 파라미터이다. 이 형태의 regularization은 Scikit-Learn의 Ridge estimator에서 사용된다.


```python
 from sklearn.linear_model import Ridge
model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))
basis_plot(model, title='Ridge Regression')
```


![ML_Basics(Linear Regression)_29_0](https://user-images.githubusercontent.com/51064261/103625383-3fcff900-4f7e-11eb-8c1e-772227543fb7.png)


$\alpha$값이 0에 가까워질수록 일반적인 선형회귀모델이 되고, $\alpha$값이 무한대로 증가하면 데이터는 모델에 영향을 주지 않게 된다.

### Lasso Regression (L1 Regularization)
또 하나의 자주 쓰이는 regularization 방법은 계수들의 절대값의 합을 제한하는 것이다.   
$ P = \alpha \sum_{n=1}^N \vert \theta_n \vert $   
뒤에서 자세히 다루겠지만 이 방법은 sparse한 모델을 생성하게 된다.(많은계수들이 0이됨)


```python
from sklearn.linear_model import Lasso
model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))
basis_plot(model, title='Lasso Regression')
```

    /Users/sumin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.002135815870488389, tolerance: 0.002065280097246271
      positive)


![ML_Basics(Linear Regression)_32_1](https://user-images.githubusercontent.com/51064261/103625384-40688f80-4f7e-11eb-9ab3-6cd08c3d27dd.png)



위에서 볼 수 있듯 대부분의 계수값들이 0이 된다. Ridge regression과 마찬가지로 $\alpha$값으로 regularization의 강도를 조절할 수 있다.

### SGD


```python
from sklearn.linear_model import SGDRegressor
model = make_pipeline(GaussianFeatures(30),
                     SGDRegressor(max_iter = 50, tol=1e-8, alpha=0))
basis_plot(model)
```

    /Users/sumin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning)



![ML_Basics(Linear Regression)_35_1](https://user-images.githubusercontent.com/51064261/103625389-40688f80-4f7e-11eb-8123-19ab1c2a7200.png)




```python

```


