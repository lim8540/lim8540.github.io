I"ü<h3 id="autograd">Autograd</h3>
<ul>
  <li>autograd íŒ¨í‚¤ì§€ëŠ” í…ì„œì˜ ëª¨ë“  ì—°ì‚°ì— ëŒ€í•œ ìë™ ë¯¸ë¶„ì„ ì œê³µ</li>
  <li>ì‹¤í–‰-ê¸°ë°˜-ì •ì˜(define-by-run) í”„ë ˆì„ì›Œí¬ë¡œ, ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì‘ì„±í•˜ì—¬ ì‹¤í–‰í•˜ëŠëƒì— ë”°ë¼ ì—­ì „íŒŒê°€ ì •ì˜ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸</li>
  <li>ì—­ì „íŒŒëŠ” í•™ìŠµ ê³¼ì •ì˜ ë§¤ ë‹¨ê³„ë§ˆë‹¤ ë‹¬ë¼ì§</li>
</ul>

<h4 id="tensor">Tensor</h4>
<ul>
  <li>
    <p>íŒ¨í‚¤ì§€ì˜ ì¤‘ì‹¬ì—ëŠ” torch.Tensor í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ .requires_grad ì†ì„±ì„ True ë¡œ ì„¤ì •í•˜ë©´, ê·¸ tensorì—ì„œ ì´ë¤„ì§„ ëª¨ë“  ì—°ì‚°ë“¤ì„ ì¶”ì (track)í•˜ê¸° ì‹œì‘í•©ë‹ˆë‹¤. ê³„ì‚°ì´ ì™„ë£Œëœ í›„ .backward() ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë“  ë³€í™”ë„(gradient)ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ Tensorì˜ ë³€í™”ë„ëŠ” .grad ì†ì„±ì— ëˆ„ì ë©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Tensorê°€ ê¸°ë¡ì„ ì¶”ì í•˜ëŠ” ê²ƒì„ ì¤‘ë‹¨í•˜ê²Œ í•˜ë ¤ë©´, .detach() ë¥¼ í˜¸ì¶œí•˜ì—¬ ì—°ì‚° ê¸°ë¡ìœ¼ë¡œë¶€í„° ë¶„ë¦¬(detach)í•˜ì—¬ ì´í›„ ì—°ì‚°ë“¤ì´ ì¶”ì ë˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>ê¸°ë¡ì„ ì¶”ì í•˜ëŠ” ê²ƒ(ê³¼ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ì½”ë“œ ë¸”ëŸ­ì„ with torch.no_grad(): ë¡œ ê°ìŒ€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ë³€í™”ë„(gradient)ëŠ” í•„ìš”ì—†ì§€ë§Œ, requires_grad=True ê°€ ì„¤ì •ë˜ì–´ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ëŠ” ëª¨ë¸ì„ í‰ê°€(evaluate)í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Autograd êµ¬í˜„ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ ë” ìˆëŠ”ë°, ì´ê²ƒì€ ë°”ë¡œ Function í´ë˜ìŠ¤ì…ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Tensor ì™€ Function ì€ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ëª¨ë“  ì—°ì‚° ê³¼ì •ì„ ë¶€í˜¸í™”(encode)í•˜ì—¬ ìˆœí™˜í•˜ì§€ ì•ŠëŠ” ê·¸ë˜í”„(acyclic graph)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê° tensorëŠ” .grad_fn ì†ì„±ì„ ê°–ê³  ìˆëŠ”ë°, ì´ëŠ” Tensor ë¥¼ ìƒì„±í•œ Function ì„ ì°¸ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ë‹¨, ì‚¬ìš©ìê°€ ë§Œë“  TensorëŠ” ì˜ˆì™¸ë¡œ, ì´ ë•Œ grad_fn ì€ None ì…ë‹ˆë‹¤.)</p>
  </li>
  <li>
    <p>ë„í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” Tensor ì˜ .backward() ë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤. ë§Œì•½ Tensor ê°€ ìŠ¤ì¹¼ë¼(scalar)ì¸ ê²½ìš°(ì˜ˆ. í•˜ë‚˜ì˜ ìš”ì†Œ ê°’ë§Œ ê°–ëŠ” ë“±)ì—ëŠ” backward ì— ì¸ìë¥¼ ì •í•´ì¤„ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ìš”ì†Œë¥¼ ê°–ê³  ìˆì„ ë•ŒëŠ” tensorì˜ ëª¨ì–‘ì„ gradient ì˜ ì¸ìë¡œ ì§€ì •í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.7.0+cu101
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># xì˜ ì—°ì‚° ê³¼ì ì„ ì¶”ì í•˜ê¸° ìœ„í•´ requires_grad=Trueë¡œ ì„¤ì •
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># ì§ì ‘ ìƒì„±í•œ Tensorì´ê¸° ë•Œë¬¸ì— grad_fnì´ Noneì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
None
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># yëŠ” ì—°ì‚°ì˜ ê²°ê³¼ë¡œ ìƒì„±ëˆ ê²ƒì´ê¸° ë•Œë¬¸ì— grad_fnì„ ê°–ê³ ìˆëŠ” ê²ƒì„ í™•ì¸ ê°€ëŠ¥
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># ì—°ì‚°ì˜ ê²°ê³¼ë¡œ ìƒì„±ëœ ê²ƒì´ê¸° ë•Œë¬¸ì— grad_fnì„ ê°–ëŠ” ê²ƒì„ í™•ì¸ ê°€ëŠ¥
</span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
&lt;AddBackward0 object at 0x7efd51925550&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># ê°ê° ì‚¬ìš©í•œ funcì— ë§ê²Œ frad_fnì´ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ
</span><span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)
tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div></div>

<ul>
  <li>requires_grad_()ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ Tensorì˜ requires_grad ê°’ì„ ë°”ê¿€ ìˆ˜ ìˆìŒ</li>
  <li>ì…ë ¥ ê°’ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê°’ì€ False</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5933,  1.4187],
        [ 1.2698, -0.5749]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[-4.3766, 10.1645],
        [14.1184,  1.0951]])
False
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[-4.3766, 10.1645],
        [14.1184,  1.0951]], requires_grad=True)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(323.0008, grad_fn=&lt;SumBackward0&gt;)
True
</code></pre></div></div>

<h4 id="ë³€í™”ë„gradient">ë³€í™”ë„(gradient)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># ì´ì „ì— ë§Œë“  outì„ ì‚¬ìš©í•´ì„œ ì—­ì „íŒŒ ì§„í–‰
</span>
<span class="n">y</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span> <span class="c1"># ì¤‘ê°„ ê°’ì— ëŒ€í•œ ë¯¸ë¶„ ê°’ì„ ë³´ê³ ì‹¶ë‹¤ë©´ í•´ë‹¹ ê°’ì— ëŒ€í•œ retain_grad()ë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
</span><span class="n">z</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="c1">#out.backward()  # ì—¬ëŸ¬ ë²ˆ ë¯¸ë¶„ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” retain_graph=Trueë¡œ ì„¤ì •í•´ì¤˜ì•¼ í•¨(ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ ë°œìƒ)
</span>
<span class="c1"># out.backward(torch.tensor(1.))ì„ ì§„í–‰í•˜ëŠ” ê²ƒê³¼ ë™ì¼
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="c1">#out.backward()
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(27., grad_fn=&lt;MeanBackward0&gt;)
None
None
None
False
None
None
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">y</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># z.retain_grad()ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ gradê°’ì„ ì €ì¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— grad ì†ì„±ì„ ë³¼ ìˆ˜ ì—†ìŒ
</span><span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(27., grad_fn=&lt;MeanBackward0&gt;)
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
None
False
tensor([[9., 9.],
        [9., 9.]])
tensor([[9., 9.],
        [9., 9.]])
None


/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  if sys.path[0] == '':
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
</code></pre></div></div>

<ul>
  <li>ì¼ë°˜ì ìœ¼ë¡œ torch.autogradëŠ” ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì„ ê³„ì‚°í•˜ëŠ” ì—”ì§„</li>
  <li>torch.autogradë¥¼ ì‚¬ìš©í•˜ë©´ ì „ì²´ ì•¼ì½”ë¹„ì•ˆì„ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì€ backwardì— í•´ë‹¹ ë²¡í„°ë¥¼ ì¸ìë¡œ ì œê³µí•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŒ</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>

<span class="k">while</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1000</span> <span class="p">:</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>

<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-1045.6127,   802.3533,  -450.6681], grad_fn=&lt;MulBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scalarê°’ì´ ì•„ë‹Œ yì˜ ë²¡í„°-ì•¼ì½”ë¹„ì•ˆ ê³±ì„ êµ¬í•˜ëŠ” ê³¼ì •
</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])
</code></pre></div></div>

<ul>
  <li>with torch.no_grad()ë¡œ ì½”ë“œ ë¸”ë¡ì„ ê°ì‹¸ì„œ autogradê°€ .requires_grad=Trueì¸ Tensorì˜ ì—°ì‚° ê¸°ë¡ì„ ì¶”ì í•˜ëŠ” ê²ƒì„ ë©ˆì¶œ ìˆ˜ë„ ìˆìŒ</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">print</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
True
False
</code></pre></div></div>

<ul>
  <li>ë˜ëŠ” .detach()ë¥¼ í˜¸ì¶œí•˜ì—¬ ë‚´ìš©ë¬¼ì€ ê°™ì§€ë§Œ requires_gradê°€ ë‹¤ë¥¸ ìƒˆë¡œìš´ í…ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nb">all</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
False
tensor(True)
</code></pre></div></div>

<h3 id="annartificial-neural-networks">ANN(Artificial Neural Networks)</h3>
<ul>
  <li>ì‹ ê²½ë§ì€ torch.nn íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•  ìˆ˜ ìˆìŒ</li>
  <li>nnì€ ëª¨ë¸ì„ ì •ì˜í•˜ê³  ë¯¸ë¶„í•˜ê¸° ìœ„í•´ì„œ ìœ„ì• ì„œ ì‚´í´ë³¸ autogradë¥¼ ì‚¬ìš©</li>
  <li>nn.Moduleì€ ê³„ì¸µ(layer)ê³¼ outputì„ ë°˜í™˜í•˜ëŠ” forward(input) ë©”ì†Œë“œë¥¼ í¬í•¨</li>
  <li>ê°„ë‹¨í•œ ìˆœì „íŒŒ ë„¤íŠ¸ì›Œí¬(feed-forward-network)</li>
  <li>ì…ë ¥ì„ ë°›ì•„ ì—¬ëŸ¬ ê³„ì¸µì— ì°¨ë¡€ë¡œ ì „ë‹¬í•œ í›„, ìµœì¢… ì¶œë ¥ì„ ì œê³µ</li>
  <li>ì‹ ê²½ë§ì˜ ì¼ë°˜ì ì¸ í•™ìŠµê³¼ì •
    <ul>
      <li>í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜)ë¥¼ ê°–ëŠ” ì‹ ê²½ë§ì„ ì •ì˜</li>
      <li>ë°ì´í„° ì…‹ ì…ë ¥ì„ ë°˜ë³µ</li>
      <li>ì…ë ¥ì„ ì‹ ê²½ë§ì—ì„œ ì „íŒŒ(process)</li>
      <li>ì†ì‹¤(loss; ì…ë ¥ ê°’ê³¼ ì˜ˆì¸¡ ê°’ê³¼ì˜ ì°¨ì´)ë¥¼ ê³„ì‚°</li>
      <li>ë³€í™”ë„(gradient)ë¥¼ ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ë“¤ì— ì—­ìœ¼ë¡œ ì „íŒŒ - ì—­ì „íŒŒ ê³¼ì •</li>
      <li>ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ 
        <ul>
          <li>ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜(weight) = ê°€ì¤‘ì¹˜(weight) - í•™ìŠµë¥ (learning rate) * ë³€í™”ë„(gredient)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">bn0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn0</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="ì†ì‹¤-í•¨ìˆ˜loss-function">ì†ì‹¤ í•¨ìˆ˜(Loss Function)</h3>
<ul>
  <li>ì†ì‹¤ í•¨ìˆ˜ëŠ”(output, target)ì„ í•œ ìŒìœ¼ë¡œ ì…ë ¥ ë°›ì•„, ì¶œë ¥ì´ ì •ë‹µìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ì¶”ì •í•˜ëŠ” ê°’ì„ ê³„ì‚°</li>
  <li>forward í•¨ìˆ˜ë§Œ ì •ì˜í•˜ê³  ë‚˜ë©´ backwardí•¨ìˆ˜ëŠ” autogradë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ì˜ë¨</li>
  <li>ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œ ë³€ìˆ˜ëŠ” net.parameters()ì— ì˜í•´ ë³€í™˜ë¨</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ëœë¤ ê°’ ìƒì„±
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">ex_X</span><span class="p">,</span> <span class="n">ex_y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ex_X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ex_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'loss : '</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">net</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'layer0.bias.grad before backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'layer0.bias.grad after backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer3</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="c1"># ì´ ë¶€ë¶„ì—ì„œ .retain_grad()ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ì´ìœ ëŠ” weightì™€ biasì˜ íŒŒë¼ë¯¸í„°ê°€ leafë…¸ë“œì´ê¸° ë•Œë¬¸
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss :  1.1834555864334106
layer0.bias.grad before backward
None
True
layer0.bias.grad after backward
tensor([-0.2200,  0.0498,  0.1702])
tensor([ 0.0787,  0.0181,  0.0594,  0.0215, -0.0098,  0.0414, -0.0756,  0.0000,
         0.0131,  0.0270, -0.0252,  0.0452,  0.0118, -0.0170,  0.0062, -0.0275])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">())</span> <span class="c1"># layer0ì˜ weight
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>16
torch.Size([128, 4])
</code></pre></div></div>

<h3 id="ê°€ì¤‘ì¹˜-ê°±ì‹ ">ê°€ì¤‘ì¹˜ ê°±ì‹ </h3>
<ul>
  <li>ê°€ì¥ ë‹¨ìˆœí•œ ê°±ì‹  ê·œì¹™ì€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë³(SGD; Stochastic Gredient Descent)</li>
  <li>ê°€ì¤‘ì¹˜(weight) = ê°€ì¤‘ì¹˜(weight) - í•™ìŠµë¥ (learning rate) * ë³€í™”ë„(gredient)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.optim íŒ¨í‚¤ì§€ì— ë‹¤ì–‘í•œ ê°±ì‹  ê·œì¹™ì´ êµ¬í˜„ë˜ì–´ ìˆìŒ
</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ex_X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ex_y</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># ì—…ë°ì´íŠ¸ ì§„í–‰
</span></code></pre></div></div>

<h3 id="mlpëª¨ë¸">MLPëª¨ë¸</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">data</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">target</span>

<span class="c1"># datasetì˜ describe
</span><span class="k">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">DESCR</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'shape of data : '</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'shape of label : '</span><span class="p">,</span> <span class="n">label</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>shape of data :  (150, 4)
shape of label :  (150,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‚˜ëˆ„ê¸°
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>112
38
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DataLoader ìƒì„±
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_train</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Net(
  (layer0): Linear(in_features=4, out_features=128, bias=True)
  (layer1): Linear(in_features=128, out_features=64, bias=True)
  (layer2): Linear(in_features=64, out_features=32, bias=True)
  (layer3): Linear(in_features=32, out_features=16, bias=True)
  (layer4): Linear(in_features=16, out_features=3, bias=True)
  (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act): ReLU()
)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">losses</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">epoch_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># output = [0.11, 0.5, 0.8] --&gt;ì˜ˆì¸¡ í´ë˜ìŠ¤ ê°’
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">epoch_accuracy</span> <span class="o">+=</span> <span class="n">accuracy</span>

  <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
  <span class="n">epoch_accuracy</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"epoch : {}, </span><span class="se">\t</span><span class="s">loss : {}, </span><span class="se">\t</span><span class="s">accuracy : {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">).</span><span class="n">zfill</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">epoch_accuracy</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>

  <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
  <span class="n">accuracies</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_accuracy</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch : 001, 	loss : 1.0535, 	accuracy : 0.4732
epoch : 002, 	loss : 1.0357, 	accuracy : 0.5179
epoch : 003, 	loss : 0.9931, 	accuracy : 0.5268
epoch : 004, 	loss : 0.9951, 	accuracy : 0.5982
epoch : 005, 	loss : 0.9665, 	accuracy : 0.5714
epoch : 006, 	loss : 0.9721, 	accuracy : 0.5804
epoch : 007, 	loss : 0.9608, 	accuracy : 0.5893
epoch : 008, 	loss : 0.9523, 	accuracy : 0.6071
epoch : 009, 	loss : 0.9384, 	accuracy : 0.5804
epoch : 010, 	loss : 0.917, 	accuracy : 0.6339
epoch : 011, 	loss : 0.9184, 	accuracy : 0.6071
epoch : 012, 	loss : 0.896, 	accuracy : 0.6339
epoch : 013, 	loss : 0.907, 	accuracy : 0.6161
epoch : 014, 	loss : 0.9229, 	accuracy : 0.5982
epoch : 015, 	loss : 0.9076, 	accuracy : 0.6161
epoch : 016, 	loss : 0.8872, 	accuracy : 0.6339
epoch : 017, 	loss : 0.9005, 	accuracy : 0.6071
epoch : 018, 	loss : 0.8822, 	accuracy : 0.625
epoch : 019, 	loss : 0.9087, 	accuracy : 0.5982
epoch : 020, 	loss : 0.8517, 	accuracy : 0.6429
epoch : 021, 	loss : 0.9115, 	accuracy : 0.5714
epoch : 022, 	loss : 0.8635, 	accuracy : 0.6339
epoch : 023, 	loss : 0.9221, 	accuracy : 0.5625
epoch : 024, 	loss : 0.8452, 	accuracy : 0.6429
epoch : 025, 	loss : 0.8372, 	accuracy : 0.6696
epoch : 026, 	loss : 0.8297, 	accuracy : 0.6875
epoch : 027, 	loss : 0.8256, 	accuracy : 0.6696
epoch : 028, 	loss : 0.7933, 	accuracy : 0.7143
epoch : 029, 	loss : 0.7757, 	accuracy : 0.7143
epoch : 030, 	loss : 0.7953, 	accuracy : 0.6964
epoch : 031, 	loss : 0.8144, 	accuracy : 0.7232
epoch : 032, 	loss : 0.7735, 	accuracy : 0.7232
epoch : 033, 	loss : 0.7741, 	accuracy : 0.7232
epoch : 034, 	loss : 0.7692, 	accuracy : 0.7857
epoch : 035, 	loss : 0.7831, 	accuracy : 0.75
epoch : 036, 	loss : 0.7693, 	accuracy : 0.7321
epoch : 037, 	loss : 0.7837, 	accuracy : 0.7232
epoch : 038, 	loss : 0.7671, 	accuracy : 0.75
epoch : 039, 	loss : 0.7944, 	accuracy : 0.7054
epoch : 040, 	loss : 0.7809, 	accuracy : 0.7054
epoch : 041, 	loss : 0.7591, 	accuracy : 0.7321
epoch : 042, 	loss : 0.7537, 	accuracy : 0.7232
epoch : 043, 	loss : 0.74, 	accuracy : 0.7589
epoch : 044, 	loss : 0.7425, 	accuracy : 0.7411
epoch : 045, 	loss : 0.6915, 	accuracy : 0.8125
epoch : 046, 	loss : 0.7291, 	accuracy : 0.7679
epoch : 047, 	loss : 0.7409, 	accuracy : 0.7768
epoch : 048, 	loss : 0.6746, 	accuracy : 0.7946
epoch : 049, 	loss : 0.6912, 	accuracy : 0.7768
epoch : 050, 	loss : 0.7614, 	accuracy : 0.7232
epoch : 051, 	loss : 0.7374, 	accuracy : 0.75
epoch : 052, 	loss : 0.6865, 	accuracy : 0.7946
epoch : 053, 	loss : 0.6923, 	accuracy : 0.7679
epoch : 054, 	loss : 0.7381, 	accuracy : 0.7232
epoch : 055, 	loss : 0.7508, 	accuracy : 0.7679
epoch : 056, 	loss : 0.6788, 	accuracy : 0.7946
epoch : 057, 	loss : 0.6153, 	accuracy : 0.8571
epoch : 058, 	loss : 0.7499, 	accuracy : 0.7679
epoch : 059, 	loss : 0.687, 	accuracy : 0.7589
epoch : 060, 	loss : 0.6258, 	accuracy : 0.8482
epoch : 061, 	loss : 0.6608, 	accuracy : 0.8036
epoch : 062, 	loss : 0.6903, 	accuracy : 0.7679
epoch : 063, 	loss : 0.7008, 	accuracy : 0.7411
epoch : 064, 	loss : 0.6564, 	accuracy : 0.8036
epoch : 065, 	loss : 0.6973, 	accuracy : 0.7768
epoch : 066, 	loss : 0.6541, 	accuracy : 0.8036
epoch : 067, 	loss : 0.6555, 	accuracy : 0.7768
epoch : 068, 	loss : 0.6798, 	accuracy : 0.7589
epoch : 069, 	loss : 0.6549, 	accuracy : 0.7857
epoch : 070, 	loss : 0.6626, 	accuracy : 0.7321
epoch : 071, 	loss : 0.6592, 	accuracy : 0.8036
epoch : 072, 	loss : 0.6734, 	accuracy : 0.7321
epoch : 073, 	loss : 0.6291, 	accuracy : 0.8214
epoch : 074, 	loss : 0.5411, 	accuracy : 0.8661
epoch : 075, 	loss : 0.5822, 	accuracy : 0.8036
epoch : 076, 	loss : 0.5977, 	accuracy : 0.8304
epoch : 077, 	loss : 0.6177, 	accuracy : 0.75
epoch : 078, 	loss : 0.5912, 	accuracy : 0.8125
epoch : 079, 	loss : 0.587, 	accuracy : 0.7679
epoch : 080, 	loss : 0.6207, 	accuracy : 0.7857
epoch : 081, 	loss : 0.6054, 	accuracy : 0.7768
epoch : 082, 	loss : 0.6047, 	accuracy : 0.7768
epoch : 083, 	loss : 0.5881, 	accuracy : 0.8125
epoch : 084, 	loss : 0.6265, 	accuracy : 0.7589
epoch : 085, 	loss : 0.5635, 	accuracy : 0.8036
epoch : 086, 	loss : 0.5832, 	accuracy : 0.7768
epoch : 087, 	loss : 0.558, 	accuracy : 0.8125
epoch : 088, 	loss : 0.6628, 	accuracy : 0.7232
epoch : 089, 	loss : 0.5938, 	accuracy : 0.7679
epoch : 090, 	loss : 0.5798, 	accuracy : 0.8125
epoch : 091, 	loss : 0.48, 	accuracy : 0.8571
epoch : 092, 	loss : 0.5079, 	accuracy : 0.8482
epoch : 093, 	loss : 0.6363, 	accuracy : 0.75
epoch : 094, 	loss : 0.568, 	accuracy : 0.7857
epoch : 095, 	loss : 0.5365, 	accuracy : 0.8393
epoch : 096, 	loss : 0.647, 	accuracy : 0.7411
epoch : 097, 	loss : 0.4984, 	accuracy : 0.8036
epoch : 098, 	loss : 0.5668, 	accuracy : 0.7946
epoch : 099, 	loss : 0.5368, 	accuracy : 0.7946
epoch : 100, 	loss : 0.4519, 	accuracy : 0.8393
epoch : 101, 	loss : 0.4946, 	accuracy : 0.8393
epoch : 102, 	loss : 0.4611, 	accuracy : 0.8571
epoch : 103, 	loss : 0.5796, 	accuracy : 0.8304
epoch : 104, 	loss : 0.4781, 	accuracy : 0.8571
epoch : 105, 	loss : 0.5898, 	accuracy : 0.7679
epoch : 106, 	loss : 0.5579, 	accuracy : 0.7946
epoch : 107, 	loss : 0.5247, 	accuracy : 0.7768
epoch : 108, 	loss : 0.5416, 	accuracy : 0.8214
epoch : 109, 	loss : 0.4824, 	accuracy : 0.8571
epoch : 110, 	loss : 0.5666, 	accuracy : 0.7946
epoch : 111, 	loss : 0.5073, 	accuracy : 0.8393
epoch : 112, 	loss : 0.5067, 	accuracy : 0.8571
epoch : 113, 	loss : 0.4825, 	accuracy : 0.8393
epoch : 114, 	loss : 0.519, 	accuracy : 0.7946
epoch : 115, 	loss : 0.5292, 	accuracy : 0.7679
epoch : 116, 	loss : 0.5681, 	accuracy : 0.7946
epoch : 117, 	loss : 0.4947, 	accuracy : 0.7946
epoch : 118, 	loss : 0.464, 	accuracy : 0.8571
epoch : 119, 	loss : 0.5706, 	accuracy : 0.7679
epoch : 120, 	loss : 0.5354, 	accuracy : 0.7946
epoch : 121, 	loss : 0.5681, 	accuracy : 0.7679
epoch : 122, 	loss : 0.4672, 	accuracy : 0.8393
epoch : 123, 	loss : 0.5121, 	accuracy : 0.7679
epoch : 124, 	loss : 0.4807, 	accuracy : 0.8482
epoch : 125, 	loss : 0.49, 	accuracy : 0.8304
epoch : 126, 	loss : 0.4438, 	accuracy : 0.8571
epoch : 127, 	loss : 0.4753, 	accuracy : 0.8304
epoch : 128, 	loss : 0.4917, 	accuracy : 0.8036
epoch : 129, 	loss : 0.5013, 	accuracy : 0.8304
epoch : 130, 	loss : 0.5482, 	accuracy : 0.7679
epoch : 131, 	loss : 0.5586, 	accuracy : 0.8036
epoch : 132, 	loss : 0.4572, 	accuracy : 0.8661
epoch : 133, 	loss : 0.4523, 	accuracy : 0.8393
epoch : 134, 	loss : 0.5082, 	accuracy : 0.7857
epoch : 135, 	loss : 0.4028, 	accuracy : 0.8839
epoch : 136, 	loss : 0.4869, 	accuracy : 0.8036
epoch : 137, 	loss : 0.476, 	accuracy : 0.8571
epoch : 138, 	loss : 0.5216, 	accuracy : 0.8125
epoch : 139, 	loss : 0.4129, 	accuracy : 0.875
epoch : 140, 	loss : 0.3732, 	accuracy : 0.8661
epoch : 141, 	loss : 0.4608, 	accuracy : 0.8393
epoch : 142, 	loss : 0.4393, 	accuracy : 0.8304
epoch : 143, 	loss : 0.4604, 	accuracy : 0.8393
epoch : 144, 	loss : 0.4713, 	accuracy : 0.8571
epoch : 145, 	loss : 0.4689, 	accuracy : 0.8393
epoch : 146, 	loss : 0.3662, 	accuracy : 0.8661
epoch : 147, 	loss : 0.4775, 	accuracy : 0.8482
epoch : 148, 	loss : 0.5035, 	accuracy : 0.8214
epoch : 149, 	loss : 0.5038, 	accuracy : 0.7768
epoch : 150, 	loss : 0.3514, 	accuracy : 0.8929
epoch : 151, 	loss : 0.5087, 	accuracy : 0.8125
epoch : 152, 	loss : 0.3676, 	accuracy : 0.8839
epoch : 153, 	loss : 0.4392, 	accuracy : 0.8304
epoch : 154, 	loss : 0.4835, 	accuracy : 0.8125
epoch : 155, 	loss : 0.4162, 	accuracy : 0.8571
epoch : 156, 	loss : 0.5106, 	accuracy : 0.8036
epoch : 157, 	loss : 0.4165, 	accuracy : 0.8482
epoch : 158, 	loss : 0.5536, 	accuracy : 0.7946
epoch : 159, 	loss : 0.4806, 	accuracy : 0.7946
epoch : 160, 	loss : 0.3597, 	accuracy : 0.875
epoch : 161, 	loss : 0.5157, 	accuracy : 0.7679
epoch : 162, 	loss : 0.4587, 	accuracy : 0.8125
epoch : 163, 	loss : 0.4411, 	accuracy : 0.8304
epoch : 164, 	loss : 0.4884, 	accuracy : 0.8393
epoch : 165, 	loss : 0.4382, 	accuracy : 0.8482
epoch : 166, 	loss : 0.4845, 	accuracy : 0.7946
epoch : 167, 	loss : 0.5069, 	accuracy : 0.7679
epoch : 168, 	loss : 0.3853, 	accuracy : 0.8393
epoch : 169, 	loss : 0.4272, 	accuracy : 0.8214
epoch : 170, 	loss : 0.48, 	accuracy : 0.7768
epoch : 171, 	loss : 0.4512, 	accuracy : 0.8393
epoch : 172, 	loss : 0.4247, 	accuracy : 0.8393
epoch : 173, 	loss : 0.4221, 	accuracy : 0.875
epoch : 174, 	loss : 0.5062, 	accuracy : 0.8036
epoch : 175, 	loss : 0.4329, 	accuracy : 0.8482
epoch : 176, 	loss : 0.581, 	accuracy : 0.7589
epoch : 177, 	loss : 0.4848, 	accuracy : 0.8125
epoch : 178, 	loss : 0.3676, 	accuracy : 0.8929
epoch : 179, 	loss : 0.5455, 	accuracy : 0.7589
epoch : 180, 	loss : 0.4271, 	accuracy : 0.8661
epoch : 181, 	loss : 0.4487, 	accuracy : 0.8125
epoch : 182, 	loss : 0.4461, 	accuracy : 0.8125
epoch : 183, 	loss : 0.4677, 	accuracy : 0.8036
epoch : 184, 	loss : 0.3864, 	accuracy : 0.8661
epoch : 185, 	loss : 0.4386, 	accuracy : 0.8482
epoch : 186, 	loss : 0.4607, 	accuracy : 0.7857
epoch : 187, 	loss : 0.5194, 	accuracy : 0.7679
epoch : 188, 	loss : 0.3284, 	accuracy : 0.8839
epoch : 189, 	loss : 0.4315, 	accuracy : 0.8304
epoch : 190, 	loss : 0.404, 	accuracy : 0.8661
epoch : 191, 	loss : 0.382, 	accuracy : 0.8571
epoch : 192, 	loss : 0.4051, 	accuracy : 0.7857
epoch : 193, 	loss : 0.4554, 	accuracy : 0.8214
epoch : 194, 	loss : 0.4782, 	accuracy : 0.7946
epoch : 195, 	loss : 0.375, 	accuracy : 0.875
epoch : 196, 	loss : 0.4547, 	accuracy : 0.8304
epoch : 197, 	loss : 0.4012, 	accuracy : 0.8482
epoch : 198, 	loss : 0.3777, 	accuracy : 0.9018
epoch : 199, 	loss : 0.4428, 	accuracy : 0.8393
epoch : 200, 	loss : 0.3521, 	accuracy : 0.875
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot result
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"$loss$"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$epochs$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"$accuracy$"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$epochs$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/image/pytorch_tutorial_autograd_%26_MLP%28Multi_layer_perceptron%29_files/pytorch_tutorial_autograd_%26_MLP%28Multi_layer_perceptron%29_38_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test
</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">((</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"test_set accuracy : "</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.return_types.max(
values=tensor([2.1466, 2.0562, 1.7744, 2.1817, 2.4490, 2.5051, 2.8734, 0.4349, 2.0243,
        2.2910, 2.3615, 1.0674, 1.5383, 2.1746, 1.9570, 0.8656, 2.3856, 2.7789,
        1.3967, 1.7692, 2.2129, 1.2555, 1.9330, 2.5707, 1.1623, 1.2368, 0.8861,
        2.1353, 2.0641, 1.9476, 2.0494, 1.9198, 2.1154, 1.4613, 1.1373, 1.5775,
        1.9589, 0.8253], grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2, 0, 0,
        2, 2, 2, 0, 0, 1, 2, 2, 0, 1, 1, 2, 0, 2]))
test_set accuracy :  0.9737
</code></pre></div></div>

:ET