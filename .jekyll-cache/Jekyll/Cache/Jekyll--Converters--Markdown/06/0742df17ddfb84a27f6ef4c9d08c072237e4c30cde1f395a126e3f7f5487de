I"P<h2 id="엔트로피">엔트로피</h2>

<ul>
  <li>자기정보(self-information) : $i(A)$
    <ul>
      <li>A : 사건</li>
      <li>$ i(A) = {log_b({1\over {P(A)}})} = -log_bP(A)$</li>
      <li>확률이 높은 사건:
        <ul>
          <li>정보가 많지 않음</li>
          <li>예 : 도둑이 들었는데 개가 짖는 경우보다 도둑이 들었는데 개가 안 짖는 경우 더 많은 정보를 포함하고 있음</li>
        </ul>
      </li>
      <li>정보의 단위
        <ul>
          <li>b = 2 : bits</li>
          <li>b = e : nats</li>
          <li>b = 10 : hartley</li>
        </ul>
      </li>
      <li>특성
        <ul>
          <li>$ i(AB) ={log_b({1\over {P(A)P(B)}})} = {log_b({1\over {P(A)}})} + {log_b({1\over {P(B)}})} = i(A) + i(B)$</li>
          <li>따라서, 두 사건이 동시에 있어났을 때, 자기정보는 각 자기정보의 합과 같다.</li>
          <li>$ P(H) = {1\over8}, P(T) = {7\over8} $
            <ul>
              <li>$ i(H) = -log_bP(H) = -log_2{1 \over 8} = 3비트$</li>
              <li>$ i(H) = -log_bP(T) = -log_2{7 \over 8} = 0.193비트$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>엔트로피(entropy)
    <ul>
      <li>자기 정보의 평균</li>
      <li>$ H(X) = \sum_J P(A_j)i(A_j) = - \sum_jP(A_j)log_2 P(A_j) $</li>
      <li>특성
        <ul>
          <li>$ 0 \le H(X) \le log_2K $ ( K : 사건의 수)
            <ul>
              <li>엔트로피 $H(X)$의 최대값은 $P(A_j)$가 모두 ${1\K}$인 경우이고 그때의 값이 $log_2K$이다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="교차-엔트로피">교차 엔트로피</h2>
<ul>
  <li>확률분포 P와 Q
    <ul>
      <li>$ S = {A_j} $(사건 S)</li>
      <li>$ P(A_j)$ : 확률분포 P에서 사건 $A_j$가 발생할 확률</li>
      <li>$ Q(A_j)$ : 확률분포 Q에서 사건 $A_j$가 발생할 확률</li>
      <li>$ i(A_j)$ : 확률분포 Q에서 사건 $A_j$의 자기정보
        <ul>
          <li>$ i(A_j) = -log_2Q(Aj)$</li>
          <li>자기정보는 $A_j$를 표현하는 비트수</li>
          <li>잘못된 확률분포 Q를 사용하게 되면, 실제 최적의 비트수를 사용하지 못하게 됨.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>$H(P,Q)$
    <ul>
      <li>집합 S상에서 확률분포 P에 대한 확률분포 Q의 교차 엔트로피</li>
      <li>확률분포 P에서 $i(A_j)$의 평균
        <ul>
          <li>$ H(P, Q) = \sum_j P(A_j)i(A_j) = - \sum_j P(A_j)log_2Q(A_j) = - \sum_{x \in X}P(X)log_2Q(x) $</li>
          <li>이 값은 정확한 확률분포 P를 사용했을 때의 비트 수 보다 크게 됨
            <ul>
              <li>$ H(P,Q) = - \sum_{x \in X}P(X)log_2Q(x) \ge - \sum_{x \in X}P(X)log_2P(x) = H(P)$</li>
            </ul>
          </li>
          <li>따라서 이 값은 P와 Q가 얼마나 비슷한지를 표현
            <ul>
              <li>같으면 $ H(P,Q) = H(P) $</li>
              <li>다르면 $ H(P,Q) &gt; H(P) $</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>분류 문제에서의 손실함수
    <ul>
      <li>분류문제
        <ul>
          <li>주어진 대상이 A인지 아닌지를 판단</li>
          <li>주어진 대상이 A, B, C, … 중 어느 것인지를 판단</li>
        </ul>
      </li>
      <li>기계학습 에서는 주어진 대상이 각 그룹에 속할 확률을 제공
        <ul>
          <li>예) [0.8, 0.2]: A일 확률 0.8, 아닐확률 0.2</li>
          <li>이 값이 정답인 [1.0 ,0.0] 과 얼마나 다른지 측정 필요</li>
        </ul>
      </li>
      <li>원하는 답 $P = [p_1, p_2, …, p_n], p_1 + p_2 + … + p_n = 1$</li>
      <li>제시된 답 $Q = [q_1, q_2, …, q_n], q_1 + q_2 + … + q_n = 1$</li>
      <li>제곱합
        <ul>
          <li>$\sum(p_i - q_i)^2$</li>
          <li>확률이 다를수록 큰 값을 가짐</li>
          <li>하지만 학습 속도 느림</li>
        </ul>
      </li>
      <li>교차 엔트로피 H(P, Q)
        <ul>
          <li>확률이 다를수록 큰 값을 가짐</li>
          <li>학습 속도 빠름</li>
          <li>분류 문제에서 주로 교차 엔트로피 사용</li>
        </ul>
      </li>
      <li>참고) 분류문제에서 원하는 답
        <ul>
          <li>$P = [p_1, p_2, …, p_n]$에서 $p_i$중 하나만 1이고, 나머지는 다 0임. 즉, 엔트로피는 0 (H(P) = 0)</li>
          <li>$p_k = 1.0$ 이라고 하면, $q_k$의 값이 최대한 커지는 방향으로 학습 진행</li>
          <li>예시)
            <ul>
              <li>S = ${A, B}$</li>
              <li>P = $[1, 0]$</li>
              <li>예측 $Q(x)$
                <ul>
                  <li>$[0.8, 0.2]: Q(A) = 0.8. Q(B) = 0.2$
                    <ul>
                      <li>$H(P, Q) = \sum_{x \in X}log_2Q(x) = -1 \times log_20.8 = 0.3219$</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

:ET