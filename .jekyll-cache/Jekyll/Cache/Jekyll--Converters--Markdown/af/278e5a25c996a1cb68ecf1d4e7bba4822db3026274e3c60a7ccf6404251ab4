I"<h2 id="결정이론-decision-theory">결정이론 (Decision Theory)</h2>

<h3 id="결정이론-이란">결정이론 이란?</h3>
<ul>
  <li>새로운 값 x가 주어졌을 때, 확률모델 $p(x,t)$에 기반해 최적의 결정(예를 들어 분류)을 내리는 것
    <ul>
      <li>추론단계 : 결합확률분포 ($p(x,C_K)$를 직접 구하는 경우도 있음). 이것만 있다면 모든 것을 할 수 있음.</li>
      <li>결정단계 : 상황에 대한 확률이 주어졌을 때 어떻게 최적의 결정을 내릴 것인지? 추론단계를 거쳤다면 결정단계는 매우 쉬움</li>
    </ul>
  </li>
  <li>예제: X-ray의 이미지로 암 판별
    <ul>
      <li>x : X-ray이미지</li>
      <li>$C_1$ : 암인 경우</li>
      <li>$C_2$ : 암이 아닌 경우</li>
      <li>$p(C_k \vert x)$의 값을 알기 원함</li>
    </ul>
  </li>
  <li>직관적으로 볼 때 $p(C_k \vert x)$를 최대화 시키는 k를 구하는 것이 좋은 결정</li>
</ul>

<h3 id="결정이론---이진분류binary-classification">결정이론 - 이진분류(Binary Classification)</h3>
<ul>
  <li>결정 영역(decision region)
    <ul>
      <li>$R_i = \{ x : pred(x) = C_i \}$</li>
    </ul>
  </li>
  <li>분류 오류 확률(probability of misclassification)
    <ul>
      <li>$p(mis) = p(x \in R_1, C_2) + p(x \in R_2, C_1) = \int_{R_1}p(x, C_2)dx + \int_{R_2}p(x, C_1)dx$</li>
    </ul>
  </li>
  <li>오류를 최소화하려면 다음조건을 만족하는 x를 $R_1$에 할당해야 한다.
    <ul>
      <li>$p(x, C_1) &gt; p(x, C_2)$</li>
      <li>$\iff p(C_1 \vert x)p(x) &gt; p(C_2 \vert x)p(x)$</li>
      <li>$\iff p(C_1 \vert x) &gt; p(C_2 \vert x)$</li>
    </ul>
  </li>
</ul>

<h3 id="multi-class일-경우">Multi Class일 경우</h3>
<ul>
  <li>$p(correct) = \sum_{k=1}^Kp(x \in R_k, C_k) = \sum_{k=1}^K\int_{R_k}p(x,C_k)dx$</li>
  <li>$ = pred(x) = argmax_kp(C_k \vert x)$</li>
</ul>

<h3 id="결정이론의-목표분류의-경우">결정이론의 목표(분류의 경우)</h3>
<ul>
  <li>결합확률분포 $p(x, C_k)$가 주어졌을 때, 최적의 결정영역들 $R_1, …, R_k$를 찾는 것.</li>
  <li>$\hat C(x)$를 $x$가 주어졌을 때 예측값(1,…,K 중의 값)을 돌려주는 함수라고 하자.
    <ul>
      <li>$x \in R_j \iff \hat C(x) = j$</li>
    </ul>
  </li>
  <li>결합확률분포 $p(x, C_k)$가 주어졌을 때, 최적의 함수 $\hat C(x)$를 찾는 것.</li>
</ul>

<h3 id="기대손실-최소화minimizing-the-expected-loss">기대손실 최소화(Minimizing the Expected Loss)</h3>
<ul>
  <li>모든 결정이 동일한 리스크를 갖는 것은 아님.
    <ul>
      <li>암이 아닌데 암인 것으로 진단</li>
      <li>암이 맞는데 암이 아닌 것으로 진단</li>
    </ul>
  </li>
  <li>손실행렬(loss matrix)
    <ul>
      <li>$L_{kj}$ : $C_k$에 속하는 $x$를 $C_j$로 분류할 때 발생하는 손실(또는 비용)</li>
      <li>데이터에 대한 모든 지식이 확률분포로 표현되고 있는 것을 기억할 것. 한 데이터샘플 x의 실제 클래스를 결정론적으로 알고 있는 것이 아니라 그것의 확률만을 알 수 있다고 가정한다. 즉, 우리가 관찰할 수 있는 샘플(예를들어, 암을 가진 환자의 X-Ray이미지)은 확률분포 $p(x, C_k)$를 통해서 생성된 것이라고 간주한다. 따라서, 손실행렬 L이 주어졌을 때, 다음과 같은 기대손실을 최소화하는 것을 목표로 할 수 있다.</li>
      <li>$E[L] = \sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx$</li>
      <li>$\hat C(x)$를 활용햐여 위의 식을 바꾸면 다음과 같다.</li>
      <li>$\int_x\sum_{k=1}^KL_{k\hat C(x)}p(x,C_k)dx = \int_x(\sum_{k=1}^KL_{k\hat C(x)}p(C_k \vert x))p(x)dx$</li>
      <li>이렇게 표현된 $E[L]$는 $\hat C(x)$의 범함수(functional)이고이 범함수를 최소화시키는 함수 $\hat C(x)$를 찾으면 된다.
        <ul>
          <li>$ \hat C(x) = argmin \sum_{k=1}^KL_{kj}p(C_k \vert x)$</li>
        </ul>
      </li>
      <li>만약에 손실행렬이 0-1 loss인 경우(주대각선 원소들은 0 나머지는 1)
        <ul>
          <li>$ \hat C(x) = argmin[ \{ \sum_{k=1}^Kp(C_k \vert x) \} - p(C_j \vert x)]$</li>
          <li>$ = argmin (1 - p(C_j \vert x)) = argmax p(C_j \vert x)$
            <h3 id="결정이론---회귀문제의-경우">결정이론 - 회귀문제의 경우</h3>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>목표값 $t \in R$</li>
  <li>손실함수 : $ L(t, y(x)) = \{y(x) - t\}^2$
    <ul>
      <li>$F[y] = E[L] = \int_R \int_X\{ y(x) - t\}^2p(x,t)dxdt$</li>
      <li>$ = \int_X (\int_R\{ y(x) - t\}^2p(x,t)dt)dx $</li>
      <li>$ = \int_X (\int_R\{ y(x) - t\}^2p(t \vert x)dt)dx $</li>
    </ul>
  </li>
</ul>
:ET