<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="Autograd autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공 실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미 역전파는 학습 과정의 매 단계마다 달라짐"> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Baesan Blog" /> <!-- Begin Jekyll SEO tag v2.6.1 --> <title>Pytorch(autograted mlp) | Baesan Blog</title> <meta name="generator" content="Jekyll v4.1.1" /> <meta property="og:title" content="Pytorch(autograted mlp)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Autograd autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공 실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미 역전파는 학습 과정의 매 단계마다 달라짐" /> <meta property="og:description" content="Autograd autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공 실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미 역전파는 학습 과정의 매 단계마다 달라짐" /> <link rel="canonical" href="http://localhost:4000/programmers/pytorch(autograted-MLP)" /> <meta property="og:url" content="http://localhost:4000/programmers/pytorch(autograted-MLP)" /> <meta property="og:site_name" content="Baesan Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2021-01-21T00:00:00+09:00" /> <script type="application/ld+json"> {"url":"http://localhost:4000/programmers/pytorch(autograted-MLP)","headline":"Pytorch(autograted mlp)","dateModified":"2021-01-21T00:00:00+09:00","datePublished":"2021-01-21T00:00:00+09:00","description":"Autograd autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공 실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미 역전파는 학습 과정의 매 단계마다 달라짐","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/programmers/pytorch(autograted-MLP)"},"@type":"BlogPosting","@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" href="/favicon.ico" type="image/icon"> <link rel="icon" href="/favicon.ico" type="image/icon"> <!-- stylesheets --> <link rel="stylesheet" type="text/css" href="/assets/css/base.css"> <link rel="stylesheet" type="text/css" href="/assets/css/simplePagination.css"> <link rel="stylesheet" type="text/css" href="/assets/css/highlight-theme.css"> <link rel="stylesheet" type="text/css" href="/assets/css/rouge-code.css"> <link rel="stylesheet" type="text/css" href="/assets/css/post.css"> <!-- javascripts --> <script type="text/javascript" src="/assets/js/jquery.js"></script> <!--[if lt IE 9]> <script src="/assets/js/html5shiv.js"></script> <![endif]--> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> </head> <!-- bare layout means not header and footer like resume --> <body> <header id="l-header"> <div class="container"> <div class="row logo"> <div class="col-lg-7"> <h1>BAESAN</h1> </div> <div class="col-lg-5"> <p>one liner one...</p> <p>one liner two...</p> </div> </div> <div class="row navicon"> <a href=""><i class="fa fa-navicon"></i></a> </div> <div class="row navbar"> <nav class="col-lg-8 col-md-8 col-xs-12"> <ul class="row"> <li class="col-lg-3"><a href="/">HOME</a></li> <li class="col-lg-3"> <ul class="subnav"> <a href="javascript:void(0)">POST</a> <li><a href="/category">CATEGORY</a></li> <li><a href="/tag">TAG</a></li> </ul> </li> <li class="col-lg-3"><a href="/series">SERIES</a></li> <li class="col-lg-3"><a href="/about">ABOUT</a></li> </ul> </nav> <div class="search col-lg-4 col-md-4 col-xs-12"> <form> <label for="search"></label> <input id="search-input" name="serach" type="text" placeholder="Search Blog Posts..."> <i class="fa fa-search"></i> </form> </div> </div> </div> </header> <section id="l-main"> <div class="container"> <div id="search-result-wrapper" class="hidden"> <h3>Search Results:</h3> <div id="search-result"></div> </div> <style> #search-result-wrapper { font-size: 2rem; background-color: #eee; padding: 5rem; padding-left: 31rem; margin-top: 1rem; margin-bottom: 1rem; box-shadow: 0 0 10px #999; border-radius: 5px; background: white; } #search-result { padding-left: 3rem; } @media (max-width: 1200px) { #search-result-wrapper { font-size: 1.7rem; padding: 2rem; } #search-result { padding-left: 1rem; } } @media (max-width: 768px) { #search-result-wrapper { margin-top: 24rem !important; } } </style> <script> $(document).ready(function() { input = $("#search-input"); wrapper = $("#search-result-wrapper"); wrapper.hide(); wrapper.removeClass("hidden"); input.on("keyup change", function() { if (! input.val()) { wrapper.slideUp("ease"); } else { wrapper.slideDown("ease"); } }); }); </script> <script src="/assets/js/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-result'), json: "/search.json" }); </script> <div class="row"> <div id="markdown-container" class="col-lg-9"> <header> <p id="post-title">Pytorch(autograted mlp)</p> <ul class="tags clearfix"> <li><i class="fa fa-tag"></i> K-digital training</li> <li><i class="fa fa-tag"></i> week7_day4</li> <li><i class="fa fa-tag"></i> ml_basics</li> <li><i class="fa fa-tag"></i> Deep Learning</li> <li><i class="fa fa-tag"></i> pytorch</li> </ul> <p id="post-meta"> posted on <b>21 Jan 2021</b> under category <b>programmers</b> </p> </header> <h3 id="autograd">Autograd</h3> <ul> <li>autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공</li> <li>실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 것을 의미</li> <li>역전파는 학습 과정의 매 단계마다 달라짐</li> </ul> <h4 id="tensor">Tensor</h4> <ul> <li> <p>패키지의 중심에는 torch.Tensor 클래스가 있습니다. 만약 .requires_grad 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작합니다. 계산이 완료된 후 .backward() 를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 .grad 속성에 누적됩니다.</p> </li> <li> <p>Tensor가 기록을 추적하는 것을 중단하게 하려면, .detach() 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.</p> </li> <li> <p>기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을 with torch.no_grad(): 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는 필요없지만, requires_grad=True 가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용합니다.</p> </li> <li> <p>Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 Function 클래스입니다.</p> </li> <li> <p>Tensor 와 Function 은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다. 각 tensor는 .grad_fn 속성을 갖고 있는데, 이는 Tensor 를 생성한 Function 을 참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 grad_fn 은 None 입니다.)</p> </li> <li> <p>도함수를 계산하기 위해서는 Tensor 의 .backward() 를 호출하면 됩니다. 만약 Tensor 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는 backward 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 gradient 의 인자로 지정할 필요가 있습니다.</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.7.0+cu101
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># x의 연산 과적을 추적하기 위해 requires_grad=True로 설정
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 직접 생성한 Tensor이기 때문에 grad_fn이 None인 것을 확인할 수 있음
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
None
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># y는 연산의 결과로 생성돈 것이기 때문에 grad_fn을 갖고있는 것을 확인 가능
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># 연산의 결과로 생성된 것이기 때문에 grad_fn을 갖는 것을 확인 가능
</span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
&lt;AddBackward0 object at 0x7efd51925550&gt;
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># 각각 사용한 func에 맞게 frad_fn이 생성된 것을 확인할 수 있음
</span><span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)
tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div></div> <ul> <li>requires_grad_()를 사용하면 기존 Tensor의 requires_grad 값을 바꿀 수 있음</li> <li>입력 값이 지정되지 않으면 기본 값은 False</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5933,  1.4187],
        [ 1.2698, -0.5749]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[-4.3766, 10.1645],
        [14.1184,  1.0951]])
False
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[-4.3766, 10.1645],
        [14.1184,  1.0951]], requires_grad=True)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(323.0008, grad_fn=&lt;SumBackward0&gt;)
True
</code></pre></div></div> <h4 id="변화도gradient">변화도(gradient)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># 이전에 만든 out을 사용해서 역전파 진행
</span>
<span class="n">y</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span> <span class="c1"># 중간 값에 대한 미분 값을 보고싶다면 해당 값에 대한 retain_grad()를 호출해야 함
</span><span class="n">z</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="c1">#out.backward()  # 여러 번 미분을 진행하기 위해서는 retain_graph=True로 설정해줘야 함(그렇지 않으면 에러 발생)
</span>
<span class="c1"># out.backward(torch.tensor(1.))을 진행하는 것과 동일
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="c1">#out.backward()
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(27., grad_fn=&lt;MeanBackward0&gt;)
None
None
None
False
None
None
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">y</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># z.retain_grad()를 호출하지 않으면 grad값을 저장하지 않기 때문에 grad 속성을 볼 수 없음
</span><span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(27., grad_fn=&lt;MeanBackward0&gt;)
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
None
False
tensor([[9., 9.],
        [9., 9.]])
tensor([[9., 9.],
        [9., 9.]])
None


/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  if sys.path[0] == '':
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
</code></pre></div></div> <ul> <li>일반적으로 torch.autograd는 벡터-야코비안 곱을 계산하는 엔진</li> <li>torch.autograd를 사용하면 전체 야코비안을 직접 계산할 수는 없지만, 벡터-야코비안 곱은 backward에 해당 벡터를 인자로 제공하여 얻을 수 있음</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>

<span class="k">while</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1000</span> <span class="p">:</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>

<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-1045.6127,   802.3533,  -450.6681], grad_fn=&lt;MulBackward0&gt;)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scalar값이 아닌 y의 벡터-야코비안 곱을 구하는 과정
</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])
</code></pre></div></div> <ul> <li>with torch.no_grad()로 코드 블록을 감싸서 autograd가 .requires_grad=True인 Tensor의 연산 기록을 추적하는 것을 멈출 수도 있음</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">print</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
True
False
</code></pre></div></div> <ul> <li>또는 .detach()를 호출하여 내용물은 같지만 requires_grad가 다른 새로운 텐서를 가져올 수 있음</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nb">all</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
False
tensor(True)
</code></pre></div></div> <h3 id="annartificial-neural-networks">ANN(Artificial Neural Networks)</h3> <ul> <li>신경망은 torch.nn 패키지를 사용하여 생성할 수 있음</li> <li>nn은 모델을 정의하고 미분하기 위해서 위애서 살펴본 autograd를 사용</li> <li>nn.Module은 계층(layer)과 output을 반환하는 forward(input) 메소드를 포함</li> <li>간단한 순전파 네트워크(feed-forward-network)</li> <li>입력을 받아 여러 계층에 차례로 전달한 후, 최종 출력을 제공</li> <li>신경망의 일반적인 학습과정 <ul> <li>학습 가능한 매개변수(가중치)를 갖는 신경망을 정의</li> <li>데이터 셋 입력을 반복</li> <li>입력을 신경망에서 전파(process)</li> <li>손실(loss; 입력 값과 예측 값과의 차이)를 계산</li> <li>변화도(gradient)를 신경망의 매개변수들에 역으로 전파 - 역전파 과정</li> <li>신경망의 가중치를 갱신 <ul> <li>새로운 가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gredient)</li> </ul> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">bn0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn0</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer0</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="손실-함수loss-function">손실 함수(Loss Function)</h3> <ul> <li>손실 함수는(output, target)을 한 쌍으로 입력 받아, 출력이 정답으로부터 얼마나 떨어져 있는지 추정하는 값을 계산</li> <li>forward 함수만 정의하고 나면 backward함수는 autograd를 사용하여 자동으로 정의됨</li> <li>모델의 학습 가능한 매개 변수는 net.parameters()에 의해 변환됨</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 랜덤 값 생성
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">ex_X</span><span class="p">,</span> <span class="n">ex_y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ex_X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ex_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'loss : '</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">net</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'layer0.bias.grad before backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">)</span>

<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'layer0.bias.grad after backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer4</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">layer3</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="c1"># 이 부분에서 .retain_grad()를 사용하지 않아도 되는 이유는 weight와 bias의 파라미터가 leaf노드이기 때문
</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss :  1.1834555864334106
layer0.bias.grad before backward
None
True
layer0.bias.grad after backward
tensor([-0.2200,  0.0498,  0.1702])
tensor([ 0.0787,  0.0181,  0.0594,  0.0215, -0.0098,  0.0414, -0.0756,  0.0000,
         0.0131,  0.0270, -0.0252,  0.0452,  0.0118, -0.0170,  0.0062, -0.0275])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">())</span> <span class="c1"># layer0의 weight
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>16
torch.Size([128, 4])
</code></pre></div></div> <h3 id="가중치-갱신">가중치 갱신</h3> <ul> <li>가장 단순한 갱신 규칙은 확률적 경사하강볍(SGD; Stochastic Gredient Descent)</li> <li>가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gredient)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.optim 패키지에 다양한 갱신 규칙이 구현되어 있음
</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ex_X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ex_y</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># 업데이트 진행
</span></code></pre></div></div> <h3 id="mlp모델">MLP모델</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">data</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">target</span>

<span class="c1"># dataset의 describe
</span><span class="k">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">DESCR</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'shape of data : '</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'shape of label : '</span><span class="p">,</span> <span class="n">label</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>shape of data :  (150, 4)
shape of label :  (150,)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 훈련과 테스트 데이터로 나누기
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>112
38
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DataLoader 생성
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_train</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Net(
  (layer0): Linear(in_features=4, out_features=128, bias=True)
  (layer1): Linear(in_features=128, out_features=64, bias=True)
  (layer2): Linear(in_features=64, out_features=32, bias=True)
  (layer3): Linear(in_features=32, out_features=16, bias=True)
  (layer4): Linear(in_features=16, out_features=3, bias=True)
  (bn0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act): ReLU()
)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">losses</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">epoch_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># output = [0.11, 0.5, 0.8] --&gt;예측 클래스 값
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">epoch_accuracy</span> <span class="o">+=</span> <span class="n">accuracy</span>

  <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
  <span class="n">epoch_accuracy</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"epoch : {}, </span><span class="se">\t</span><span class="s">loss : {}, </span><span class="se">\t</span><span class="s">accuracy : {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">).</span><span class="n">zfill</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">epoch_accuracy</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>

  <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
  <span class="n">accuracies</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_accuracy</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch : 001, 	loss : 1.0535, 	accuracy : 0.4732
epoch : 002, 	loss : 1.0357, 	accuracy : 0.5179
epoch : 003, 	loss : 0.9931, 	accuracy : 0.5268
epoch : 004, 	loss : 0.9951, 	accuracy : 0.5982
epoch : 005, 	loss : 0.9665, 	accuracy : 0.5714
epoch : 006, 	loss : 0.9721, 	accuracy : 0.5804
epoch : 007, 	loss : 0.9608, 	accuracy : 0.5893
epoch : 008, 	loss : 0.9523, 	accuracy : 0.6071
epoch : 009, 	loss : 0.9384, 	accuracy : 0.5804
epoch : 010, 	loss : 0.917, 	accuracy : 0.6339
epoch : 011, 	loss : 0.9184, 	accuracy : 0.6071
epoch : 012, 	loss : 0.896, 	accuracy : 0.6339
epoch : 013, 	loss : 0.907, 	accuracy : 0.6161
epoch : 014, 	loss : 0.9229, 	accuracy : 0.5982
epoch : 015, 	loss : 0.9076, 	accuracy : 0.6161
epoch : 016, 	loss : 0.8872, 	accuracy : 0.6339
epoch : 017, 	loss : 0.9005, 	accuracy : 0.6071
epoch : 018, 	loss : 0.8822, 	accuracy : 0.625
epoch : 019, 	loss : 0.9087, 	accuracy : 0.5982
epoch : 020, 	loss : 0.8517, 	accuracy : 0.6429
epoch : 021, 	loss : 0.9115, 	accuracy : 0.5714
epoch : 022, 	loss : 0.8635, 	accuracy : 0.6339
epoch : 023, 	loss : 0.9221, 	accuracy : 0.5625
epoch : 024, 	loss : 0.8452, 	accuracy : 0.6429
epoch : 025, 	loss : 0.8372, 	accuracy : 0.6696
epoch : 026, 	loss : 0.8297, 	accuracy : 0.6875
epoch : 027, 	loss : 0.8256, 	accuracy : 0.6696
epoch : 028, 	loss : 0.7933, 	accuracy : 0.7143
epoch : 029, 	loss : 0.7757, 	accuracy : 0.7143
epoch : 030, 	loss : 0.7953, 	accuracy : 0.6964
epoch : 031, 	loss : 0.8144, 	accuracy : 0.7232
epoch : 032, 	loss : 0.7735, 	accuracy : 0.7232
epoch : 033, 	loss : 0.7741, 	accuracy : 0.7232
epoch : 034, 	loss : 0.7692, 	accuracy : 0.7857
epoch : 035, 	loss : 0.7831, 	accuracy : 0.75
epoch : 036, 	loss : 0.7693, 	accuracy : 0.7321
epoch : 037, 	loss : 0.7837, 	accuracy : 0.7232
epoch : 038, 	loss : 0.7671, 	accuracy : 0.75
epoch : 039, 	loss : 0.7944, 	accuracy : 0.7054
epoch : 040, 	loss : 0.7809, 	accuracy : 0.7054
epoch : 041, 	loss : 0.7591, 	accuracy : 0.7321
epoch : 042, 	loss : 0.7537, 	accuracy : 0.7232
epoch : 043, 	loss : 0.74, 	accuracy : 0.7589
epoch : 044, 	loss : 0.7425, 	accuracy : 0.7411
epoch : 045, 	loss : 0.6915, 	accuracy : 0.8125
epoch : 046, 	loss : 0.7291, 	accuracy : 0.7679
epoch : 047, 	loss : 0.7409, 	accuracy : 0.7768
epoch : 048, 	loss : 0.6746, 	accuracy : 0.7946
epoch : 049, 	loss : 0.6912, 	accuracy : 0.7768
epoch : 050, 	loss : 0.7614, 	accuracy : 0.7232
epoch : 051, 	loss : 0.7374, 	accuracy : 0.75
epoch : 052, 	loss : 0.6865, 	accuracy : 0.7946
epoch : 053, 	loss : 0.6923, 	accuracy : 0.7679
epoch : 054, 	loss : 0.7381, 	accuracy : 0.7232
epoch : 055, 	loss : 0.7508, 	accuracy : 0.7679
epoch : 056, 	loss : 0.6788, 	accuracy : 0.7946
epoch : 057, 	loss : 0.6153, 	accuracy : 0.8571
epoch : 058, 	loss : 0.7499, 	accuracy : 0.7679
epoch : 059, 	loss : 0.687, 	accuracy : 0.7589
epoch : 060, 	loss : 0.6258, 	accuracy : 0.8482
epoch : 061, 	loss : 0.6608, 	accuracy : 0.8036
epoch : 062, 	loss : 0.6903, 	accuracy : 0.7679
epoch : 063, 	loss : 0.7008, 	accuracy : 0.7411
epoch : 064, 	loss : 0.6564, 	accuracy : 0.8036
epoch : 065, 	loss : 0.6973, 	accuracy : 0.7768
epoch : 066, 	loss : 0.6541, 	accuracy : 0.8036
epoch : 067, 	loss : 0.6555, 	accuracy : 0.7768
epoch : 068, 	loss : 0.6798, 	accuracy : 0.7589
epoch : 069, 	loss : 0.6549, 	accuracy : 0.7857
epoch : 070, 	loss : 0.6626, 	accuracy : 0.7321
epoch : 071, 	loss : 0.6592, 	accuracy : 0.8036
epoch : 072, 	loss : 0.6734, 	accuracy : 0.7321
epoch : 073, 	loss : 0.6291, 	accuracy : 0.8214
epoch : 074, 	loss : 0.5411, 	accuracy : 0.8661
epoch : 075, 	loss : 0.5822, 	accuracy : 0.8036
epoch : 076, 	loss : 0.5977, 	accuracy : 0.8304
epoch : 077, 	loss : 0.6177, 	accuracy : 0.75
epoch : 078, 	loss : 0.5912, 	accuracy : 0.8125
epoch : 079, 	loss : 0.587, 	accuracy : 0.7679
epoch : 080, 	loss : 0.6207, 	accuracy : 0.7857
epoch : 081, 	loss : 0.6054, 	accuracy : 0.7768
epoch : 082, 	loss : 0.6047, 	accuracy : 0.7768
epoch : 083, 	loss : 0.5881, 	accuracy : 0.8125
epoch : 084, 	loss : 0.6265, 	accuracy : 0.7589
epoch : 085, 	loss : 0.5635, 	accuracy : 0.8036
epoch : 086, 	loss : 0.5832, 	accuracy : 0.7768
epoch : 087, 	loss : 0.558, 	accuracy : 0.8125
epoch : 088, 	loss : 0.6628, 	accuracy : 0.7232
epoch : 089, 	loss : 0.5938, 	accuracy : 0.7679
epoch : 090, 	loss : 0.5798, 	accuracy : 0.8125
epoch : 091, 	loss : 0.48, 	accuracy : 0.8571
epoch : 092, 	loss : 0.5079, 	accuracy : 0.8482
epoch : 093, 	loss : 0.6363, 	accuracy : 0.75
epoch : 094, 	loss : 0.568, 	accuracy : 0.7857
epoch : 095, 	loss : 0.5365, 	accuracy : 0.8393
epoch : 096, 	loss : 0.647, 	accuracy : 0.7411
epoch : 097, 	loss : 0.4984, 	accuracy : 0.8036
epoch : 098, 	loss : 0.5668, 	accuracy : 0.7946
epoch : 099, 	loss : 0.5368, 	accuracy : 0.7946
epoch : 100, 	loss : 0.4519, 	accuracy : 0.8393
epoch : 101, 	loss : 0.4946, 	accuracy : 0.8393
epoch : 102, 	loss : 0.4611, 	accuracy : 0.8571
epoch : 103, 	loss : 0.5796, 	accuracy : 0.8304
epoch : 104, 	loss : 0.4781, 	accuracy : 0.8571
epoch : 105, 	loss : 0.5898, 	accuracy : 0.7679
epoch : 106, 	loss : 0.5579, 	accuracy : 0.7946
epoch : 107, 	loss : 0.5247, 	accuracy : 0.7768
epoch : 108, 	loss : 0.5416, 	accuracy : 0.8214
epoch : 109, 	loss : 0.4824, 	accuracy : 0.8571
epoch : 110, 	loss : 0.5666, 	accuracy : 0.7946
epoch : 111, 	loss : 0.5073, 	accuracy : 0.8393
epoch : 112, 	loss : 0.5067, 	accuracy : 0.8571
epoch : 113, 	loss : 0.4825, 	accuracy : 0.8393
epoch : 114, 	loss : 0.519, 	accuracy : 0.7946
epoch : 115, 	loss : 0.5292, 	accuracy : 0.7679
epoch : 116, 	loss : 0.5681, 	accuracy : 0.7946
epoch : 117, 	loss : 0.4947, 	accuracy : 0.7946
epoch : 118, 	loss : 0.464, 	accuracy : 0.8571
epoch : 119, 	loss : 0.5706, 	accuracy : 0.7679
epoch : 120, 	loss : 0.5354, 	accuracy : 0.7946
epoch : 121, 	loss : 0.5681, 	accuracy : 0.7679
epoch : 122, 	loss : 0.4672, 	accuracy : 0.8393
epoch : 123, 	loss : 0.5121, 	accuracy : 0.7679
epoch : 124, 	loss : 0.4807, 	accuracy : 0.8482
epoch : 125, 	loss : 0.49, 	accuracy : 0.8304
epoch : 126, 	loss : 0.4438, 	accuracy : 0.8571
epoch : 127, 	loss : 0.4753, 	accuracy : 0.8304
epoch : 128, 	loss : 0.4917, 	accuracy : 0.8036
epoch : 129, 	loss : 0.5013, 	accuracy : 0.8304
epoch : 130, 	loss : 0.5482, 	accuracy : 0.7679
epoch : 131, 	loss : 0.5586, 	accuracy : 0.8036
epoch : 132, 	loss : 0.4572, 	accuracy : 0.8661
epoch : 133, 	loss : 0.4523, 	accuracy : 0.8393
epoch : 134, 	loss : 0.5082, 	accuracy : 0.7857
epoch : 135, 	loss : 0.4028, 	accuracy : 0.8839
epoch : 136, 	loss : 0.4869, 	accuracy : 0.8036
epoch : 137, 	loss : 0.476, 	accuracy : 0.8571
epoch : 138, 	loss : 0.5216, 	accuracy : 0.8125
epoch : 139, 	loss : 0.4129, 	accuracy : 0.875
epoch : 140, 	loss : 0.3732, 	accuracy : 0.8661
epoch : 141, 	loss : 0.4608, 	accuracy : 0.8393
epoch : 142, 	loss : 0.4393, 	accuracy : 0.8304
epoch : 143, 	loss : 0.4604, 	accuracy : 0.8393
epoch : 144, 	loss : 0.4713, 	accuracy : 0.8571
epoch : 145, 	loss : 0.4689, 	accuracy : 0.8393
epoch : 146, 	loss : 0.3662, 	accuracy : 0.8661
epoch : 147, 	loss : 0.4775, 	accuracy : 0.8482
epoch : 148, 	loss : 0.5035, 	accuracy : 0.8214
epoch : 149, 	loss : 0.5038, 	accuracy : 0.7768
epoch : 150, 	loss : 0.3514, 	accuracy : 0.8929
epoch : 151, 	loss : 0.5087, 	accuracy : 0.8125
epoch : 152, 	loss : 0.3676, 	accuracy : 0.8839
epoch : 153, 	loss : 0.4392, 	accuracy : 0.8304
epoch : 154, 	loss : 0.4835, 	accuracy : 0.8125
epoch : 155, 	loss : 0.4162, 	accuracy : 0.8571
epoch : 156, 	loss : 0.5106, 	accuracy : 0.8036
epoch : 157, 	loss : 0.4165, 	accuracy : 0.8482
epoch : 158, 	loss : 0.5536, 	accuracy : 0.7946
epoch : 159, 	loss : 0.4806, 	accuracy : 0.7946
epoch : 160, 	loss : 0.3597, 	accuracy : 0.875
epoch : 161, 	loss : 0.5157, 	accuracy : 0.7679
epoch : 162, 	loss : 0.4587, 	accuracy : 0.8125
epoch : 163, 	loss : 0.4411, 	accuracy : 0.8304
epoch : 164, 	loss : 0.4884, 	accuracy : 0.8393
epoch : 165, 	loss : 0.4382, 	accuracy : 0.8482
epoch : 166, 	loss : 0.4845, 	accuracy : 0.7946
epoch : 167, 	loss : 0.5069, 	accuracy : 0.7679
epoch : 168, 	loss : 0.3853, 	accuracy : 0.8393
epoch : 169, 	loss : 0.4272, 	accuracy : 0.8214
epoch : 170, 	loss : 0.48, 	accuracy : 0.7768
epoch : 171, 	loss : 0.4512, 	accuracy : 0.8393
epoch : 172, 	loss : 0.4247, 	accuracy : 0.8393
epoch : 173, 	loss : 0.4221, 	accuracy : 0.875
epoch : 174, 	loss : 0.5062, 	accuracy : 0.8036
epoch : 175, 	loss : 0.4329, 	accuracy : 0.8482
epoch : 176, 	loss : 0.581, 	accuracy : 0.7589
epoch : 177, 	loss : 0.4848, 	accuracy : 0.8125
epoch : 178, 	loss : 0.3676, 	accuracy : 0.8929
epoch : 179, 	loss : 0.5455, 	accuracy : 0.7589
epoch : 180, 	loss : 0.4271, 	accuracy : 0.8661
epoch : 181, 	loss : 0.4487, 	accuracy : 0.8125
epoch : 182, 	loss : 0.4461, 	accuracy : 0.8125
epoch : 183, 	loss : 0.4677, 	accuracy : 0.8036
epoch : 184, 	loss : 0.3864, 	accuracy : 0.8661
epoch : 185, 	loss : 0.4386, 	accuracy : 0.8482
epoch : 186, 	loss : 0.4607, 	accuracy : 0.7857
epoch : 187, 	loss : 0.5194, 	accuracy : 0.7679
epoch : 188, 	loss : 0.3284, 	accuracy : 0.8839
epoch : 189, 	loss : 0.4315, 	accuracy : 0.8304
epoch : 190, 	loss : 0.404, 	accuracy : 0.8661
epoch : 191, 	loss : 0.382, 	accuracy : 0.8571
epoch : 192, 	loss : 0.4051, 	accuracy : 0.7857
epoch : 193, 	loss : 0.4554, 	accuracy : 0.8214
epoch : 194, 	loss : 0.4782, 	accuracy : 0.7946
epoch : 195, 	loss : 0.375, 	accuracy : 0.875
epoch : 196, 	loss : 0.4547, 	accuracy : 0.8304
epoch : 197, 	loss : 0.4012, 	accuracy : 0.8482
epoch : 198, 	loss : 0.3777, 	accuracy : 0.9018
epoch : 199, 	loss : 0.4428, 	accuracy : 0.8393
epoch : 200, 	loss : 0.3521, 	accuracy : 0.875
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot result
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"$loss$"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$epochs$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"$accuracy$"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$epochs$"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/image/pytorch_tutorial_autograd_%26_MLP%28Multi_layer_perceptron%29_files/pytorch_tutorial_autograd_%26_MLP%28Multi_layer_perceptron%29_38_0.png" alt="png" /></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test
</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">((</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"test_set accuracy : "</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.return_types.max(
values=tensor([2.1466, 2.0562, 1.7744, 2.1817, 2.4490, 2.5051, 2.8734, 0.4349, 2.0243,
        2.2910, 2.3615, 1.0674, 1.5383, 2.1746, 1.9570, 0.8656, 2.3856, 2.7789,
        1.3967, 1.7692, 2.2129, 1.2555, 1.9330, 2.5707, 1.1623, 1.2368, 0.8861,
        2.1353, 2.0641, 1.9476, 2.0494, 1.9198, 2.1154, 1.4613, 1.1373, 1.5775,
        1.9589, 0.8253], grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2, 0, 0,
        2, 2, 2, 0, 0, 1, 2, 2, 0, 1, 1, 2, 0, 2]))
test_set accuracy :  0.9737
</code></pre></div></div> </div> <div id="markdown-outline" class="col-lg-3"> </div> </div> </div> </section> <footer id="l-footer"> <div class="container"> <div class="row"> <div id="social" class="col-lg-5 col-md-5 col-sm-12"> <h3>SOCIAL</h3> <ul> <li> <i class="fa fa-github-square" title="GitHub"></i> <a href="https://github.com">&nbsp;&nbsp;https://github.com</a> </li> </ul> </div> <div id="contact" class="col-lg-4 col-md-4 col-sm-12"> <h3>CONTACT</h3> <ul> <li> <i class="fa fa-phone-square" title="Mobile"></i> <a href="tel: 123 4567 8010">&nbsp;&nbsp;123 4567 8910</a> </li> <li> <i class="fa fa-envelope" title="Email"></i> <a href="mailto: example@example.com">&nbsp;&nbsp;example@example.com</a> </li> </ul> </div> <div id="rss" class="col-lg-3 col-md-3 col-sm-12"> <h3>SUBSCRIBE</h3> <a href="/feed.xml"> <i class="rss fa fa-rss-square"></i> </a> </div> </div> <p id="legal"> Copyright (c) 2021 YOUR NAME | Powered by <a href="http://jekyllrb.com">Jekyll</a> &amp; <a href="http://github.com">GitHub</a> | designed &amp; build by <a href="http://unifreak.github.io">UniFreak</a><br /> The blog posts on this site are licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. </p> </div> </footer> <script type="text/javascript" src="/assets/js/base.js"></script> <!-- Global site tag (gtag.js) - Google Analytics --> <div id="search-result-wrapper" class="hidden"> <h3>Search Results:</h3> <div id="search-result"></div> </div> <style> #search-result-wrapper { font-size: 2rem; background-color: #eee; padding: 5rem; padding-left: 31rem; margin-top: 1rem; margin-bottom: 1rem; box-shadow: 0 0 10px #999; border-radius: 5px; background: white; } #search-result { padding-left: 3rem; } @media (max-width: 1200px) { #search-result-wrapper { font-size: 1.7rem; padding: 2rem; } #search-result { padding-left: 1rem; } } @media (max-width: 768px) { #search-result-wrapper { margin-top: 24rem !important; } } </style> <script> $(document).ready(function() { input = $("#search-input"); wrapper = $("#search-result-wrapper"); wrapper.hide(); wrapper.removeClass("hidden"); input.on("keyup change", function() { if (! input.val()) { wrapper.slideUp("ease"); } else { wrapper.slideDown("ease"); } }); }); </script> <script src="/assets/js/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-result'), json: "/search.json" }); </script> <script type="text/javascript" src="/assets/js/post.js"></script> </body> </html>
